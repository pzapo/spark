{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Master thesis\n",
    "Methods for economic big data analysis\n",
    "@Adam Wiszniewski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####0. Test Spark Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print \"This was last run on: %s\" % datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sc.parallelize([\"hello\", \"world\", \"goodbye\", \"hello\", \"again\"])\n",
    "wordcounts = words.map(lambda s: (s, 1)).reduceByKey(lambda a, b : a + b).collect()\n",
    "wordcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####1. ETL - Data loading to cluster\n",
    "In this step we are loading data to objects and preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load Stock data to DataFrame and apply schema on it to enable computation\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col,udf, unix_timestamp\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "def loadStockDatasetToDF(datasetPath):\n",
    "\n",
    "  #prepare a basic schema for stock data\n",
    "    customSchema = StructType([ \\\n",
    "    StructField(\"Date\", StringType(), True), \\\n",
    "    StructField(\"Open\", FloatType(), True), \\\n",
    "    StructField(\"High\", FloatType(), True), \\\n",
    "    StructField(\"Low\", FloatType(), True), \\\n",
    "    StructField(\"Close\", FloatType(), True), \\\n",
    "    StructField(\"Volume\", FloatType(), True)])\n",
    "  \n",
    "  #load data from csv file\n",
    "    df = sqlContext.read.format(\"csv\").load(datasetPath,\n",
    "                    format='com.databricks.spark.csv', \n",
    "                    header='true', \n",
    "                    schema=customSchema)\n",
    "  #apply a proper format using udf function\n",
    "    func =  udf (lambda x: datetime.strptime(x, '%d-%b-%y'), DateType())\n",
    "    df = df.withColumn('DateNew', func(col('Date'))).drop('Date')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load Forex data to DataFrame and apply schema on it to enable computation\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col,udf, unix_timestamp\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "def loadForexDatasetToDF(datasetPath):\n",
    "\n",
    "  #prepare a basic schema for stock data\n",
    "    customSchema = StructType([ \\\n",
    "    StructField(\"Date\", StringType(), True), \\\n",
    "    StructField(\"Close\", FloatType(), True), \\\n",
    "    StructField(\"Open\", FloatType(), True), \\\n",
    "    StructField(\"High\", FloatType(), True), \\\n",
    "    StructField(\"Low\", FloatType(), True), \\\n",
    "    StructField(\"Change\", StringType(), True)])\n",
    "  \n",
    "  #load data from csv file\n",
    "    df = sqlContext.read.format(\"csv\").load(datasetPath,\n",
    "                    format='com.databricks.spark.csv', \n",
    "                    header='true', \n",
    "                    schema=customSchema)\n",
    "  #apply a proper format using udf function\n",
    "    func =  udf (lambda x: datetime.strptime(x, '%d-%b-%y'), DateType())\n",
    "    df = df.withColumn('DateNew', func(col('Date'))).drop('Date')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "#Function preprocess Forex data and returns dataframe with Date, Open, Close and DailyChange\n",
    "def forexPreprocess(ForexDataFrame):\n",
    "  df = ForexDataFrame.select(ForexDataFrame.DateNew,ForexDataFrame.Open.alias(\"ForexOpen\"),ForexDataFrame.Close.alias(\"ForexClose\"))\n",
    "  df = df.withColumn('ForexChange', ((df.ForexClose / df.ForexOpen)*100-100)\n",
    "                .cast(\"float\")).select('DateNew','ForexOpen','ForexClose',format_number('ForexChange',2).alias('ForexDailyChangePrc').cast(\"float\"))\n",
    "  return df\n",
    "\n",
    "#Function preprocess Stock data and returns dataframe with Date, Open, Close and DailyChange\n",
    "def stockPreprocess(StockDataFrame):\n",
    "  df = StockDataFrame.select(StockDataFrame.DateNew,StockDataFrame.Volume.alias('StockVolume'), StockDataFrame.Open.alias(\"StockOpen\"),StockDataFrame.Close.alias(\"StockClose\"))\n",
    "  df = df.withColumn('StockChange', ((df.StockClose / df.StockOpen)*100-100)\n",
    "                .cast(\"float\")).select('DateNew','StockOpen','StockClose',format_number('StockChange',2).alias('StockDailyChangePrc').cast(\"float\"), 'StockVolume')\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinTwoDataFrames(df1,df2):\n",
    "  return df1.join(df2,'DateNew','inner').sort('DateNew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def calculateChangeOvernight(joinedDF):\n",
    "  wSpec = Window.partitionBy().orderBy(col(\"DateNew\"))\n",
    "  #dziala, dodaj to do jednej ramki i zrob działanie pokazujące % zmianę dnia poprzedniego\n",
    "  df = joinedDF.withColumn('PrevDayStockClose',lag(joinedDF['StockClose']).over(wSpec)).withColumn('PrevDayForexClose',lag(joinedDF['ForexClose']).over(wSpec))\n",
    "  df = df.withColumn('StockCloseToOpenChangePrc',((df.StockOpen / df.PrevDayStockClose)*100-100).cast(\"double\")).withColumn('ForexCloseToOpenChangePrc',((df.ForexOpen / df.PrevDayForexClose)*100-100).cast(\"float\"))\n",
    "  df = df.drop('PrevDayStockClose').drop('PrevDayForexClose')\n",
    "  df = df.withColumn('ForexDailyUp',df.ForexDailyChangePrc>0).withColumn('ForexOvernightUp',(df.ForexCloseToOpenChangePrc>0).cast('integer')).withColumn('StockOvernightUp',(df.StockCloseToOpenChangePrc>0).cast('integer'))\n",
    "  df = df.select('DateNew','StockVolume','StockOpen','StockClose','StockDailyChangePrc',format_number('StockCloseToOpenChangePrc',2).alias('StockCloseToOpenChangePrc').cast(\"double\"),'ForexOpen','ForexClose','ForexDailyChangePrc',format_number('ForexCloseToOpenChangePrc',2).alias('ForexCloseToOpenChangePrc').cast(\"float\"),'ForexOvernightUp','StockOvernightUp' )\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data to the lists objects to easy iterate on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populate data location\n",
    "currencyListLocation = ['/FileStore/tables/suarsxcm1478429136036/USDCHF_DAILY.csv'];\n",
    "\n",
    "swissCompListLocation = [\n",
    "'/FileStore/tables/d1xhfgxz1480700568620/swiss_abb.csv'\n",
    ",'/FileStore/tables/suarsxcm1478429136036/swiss_cs.csv'\n",
    ",'/FileStore/tables/suarsxcm1478429136036/swiss_nvs.csv'\n",
    ",'/FileStore/tables/suarsxcm1478429136036/swiss_syt.csv'\n",
    ",'/FileStore/tables/suarsxcm1478429136036/swiss_ubs.csv'];\n",
    "\n",
    "nonSwissCompListLocation = [\n",
    "'/FileStore/tables/suarsxcm1478429136036/jnj.csv'\n",
    ",'/FileStore/tables/suarsxcm1478429136036/jpm.csv'\n",
    ",'/FileStore/tables/suarsxcm1478429136036/ko.csv'\n",
    ",'/FileStore/tables/suarsxcm1478429136036/pg.csv'\n",
    ",'/FileStore/tables/suarsxcm1478429136036/ge.csv'\n",
    ",'/FileStore/tables/l893vb431480695818503/baba.csv'\n",
    ",'/FileStore/tables/l893vb431480695818503/bac.csv'\n",
    ",'/FileStore/tables/l893vb431480695818503/cvx.csv'\n",
    ",'/FileStore/tables/l893vb431480695818503/orcl.csv'\n",
    ",'/FileStore/tables/l893vb431480695818503/p.csv'\n",
    ",'/FileStore/tables/l893vb431480695818503/pep.csv'\n",
    ",'/FileStore/tables/l893vb431480695818503/t.csv'\n",
    ",'/FileStore/tables/l893vb431480695818503/wfc.csv'\n",
    ",'/FileStore/tables/l893vb431480695818503/xom.csv'];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataName(Object):\n",
    "  return Object[40:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "def buildDataFramesDict(LocationList,currencyNumber):\n",
    "  d={}\n",
    "  for x in LocationList:\n",
    "    d[getDataName(x)]= calculateChangeOvernight(joinTwoDataFrames(stockPreprocess(loadStockDatasetToDF(x))\\\n",
    "                      ,forexPreprocess(loadForexDatasetToDF(currencyListLocation[currencyNumber]))))\n",
    "  return d\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Dictionary Object which will contain (key=stock symbol;value = Dataframe with stock and forex combined )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "swissCompDict = buildDataFramesDict(swissCompListLocation,0)\n",
    "nonSwissCompDict = buildDataFramesDict(nonSwissCompListLocation,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(swissCompDict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = loadForexDatasetToDF('/FileStore/tables/suarsxcm1478429136036/USDCHF_DAILY.csv')\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nonSwissCompDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = swissCompDict[\"swiss_abb\"].na.drop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nonSwissCompDict[\"pg\"].na.drop()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = forexPreprocess(loadForexDatasetToDF('/FileStore/tables/suarsxcm1478429136036/USDCHF_DAILY.csv'));\n",
    "df_f.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_f.stat.freqItems([\"ForexOpen\",\"ForexClose\",\"ForexDailyChangePrc\"],0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4. Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean nulls\n",
    "dataset = df.na.drop()\n",
    "cols = dataset.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "categoricalColumns = [\"ForexOvernightUp\"]\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in categoricalColumns:\n",
    "  # Category Indexing with StringIndexer\n",
    "  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n",
    "  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n",
    "  # Add stages.  These are not run here, but will run all at once later on.\n",
    "  stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol = \"StockOvernightUp\", outputCol = \"label\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all features into a vector using VectorAssembler\n",
    "\n",
    "numericCols = [\"StockVolume\", \"StockOpen\", \"StockClose\", \"StockDailyChangePrc\", \"ForexOpen\",\"ForexClose\", \"ForexDailyChangePrc\"]\n",
    "assemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "# Run the feature transformations.\n",
    "#  - fit() computes feature statistics as needed.\n",
    "#  - transform() actually transforms the features.\n",
    "pipelineModel = pipeline.fit(dataset)\n",
    "dataset = pipelineModel.transform(dataset)\n",
    "\n",
    "# Keep relevant columns\n",
    "selectedcols = [\"label\", \"features\"] + cols\n",
    "dataset = dataset.select(selectedcols)\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "print trainingData.count()\n",
    "print testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "# LogisticRegression.transform() will only use the 'features' column.\n",
    "predictions = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.count()\n",
    "predictions.where(\"prediction = 0 AND label=1\").count()\n",
    "predictions.where(\"prediction = 1 AND label=0\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = predictions.select(\"label\", \"prediction\", \"ForexOpen\", \"ForexClose\", \"ForexCloseToOpenChangePrc\")\n",
    "display(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://gim.unmc.edu/dxtests/roc3.htm - .90-1 = excellent (A)\n",
    ".80-.90 = good (B)\n",
    ".70-.80 = fair (C)\n",
    ".60-.70 = poor (D)\n",
    ".50-.60 = fail (F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.2 Tuning of logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "print lr.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(trainingData)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Generic model  which allows us to test our dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "def trainMLModel(MLReadyDataFrame, trainingData):\n",
    "  # Create initial LogisticRegression model\n",
    "  lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "  # Train model with Training Data\n",
    "  lrModel = lr.fit(trainingData)\n",
    "  return lrModel\n",
    "\n",
    "def testMLModel(trainedModel, testData):\n",
    "  # Make predictions on test data using the transform() method.\n",
    "  # LogisticRegression.transform() will only use the 'features' column.\n",
    "  predictions = lrModel.transform(testData)\n",
    "  return predictions\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "def evaluateMLModel(predictions)\n",
    "  # Evaluate model\n",
    "  evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "  return evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "def buildInputMLObject(dataset):\n",
    "  categoricalColumns = [\"ForexOvernightUp\"]\n",
    "  stages = [] # stages in our Pipeline\n",
    "  for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "  # Convert label into label indices using the StringIndexer\n",
    "  label_stringIdx = StringIndexer(inputCol = \"StockOvernightUp\", outputCol = \"label\")\n",
    "  stages += [label_stringIdx]\n",
    "\n",
    "  # Transform all features into a vector using VectorAssembler\n",
    "\n",
    "  numericCols = [\"StockVolume\", \"StockOpen\", \"StockClose\", \"StockDailyChangePrc\", \"ForexOpen\",\"ForexClose\", \"ForexDailyChangePrc\"]\n",
    "  assemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\n",
    "  assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "  stages += [assembler]\n",
    "\n",
    "  # Create a Pipeline.\n",
    "  pipeline = Pipeline(stages=stages)\n",
    "  # Run the feature transformations.\n",
    "  #  - fit() computes feature statistics as needed.\n",
    "  #  - transform() actually transforms the features.\n",
    "  pipelineModel = pipeline.fit(dataset)\n",
    "  dataset = pipelineModel.transform(dataset)\n",
    "\n",
    "  # Keep relevant columns\n",
    "  selectedcols = [\"label\", \"features\"] + cols\n",
    "  MLReadyDataFrame = dataset.select(selectedcols)\n",
    "  return MLReadyDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "def trainMLModel(MLReadyDataFrame, trainingData):\n",
    "  # Create initial LogisticRegression model\n",
    "  lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "  # Train model with Training Data\n",
    "  lrModel = lr.fit(trainingData)\n",
    "  return lrModel\n",
    "\n",
    "def testMLModel(trainedModel, testData):\n",
    "  # Make predictions on test data using the transform() method.\n",
    "  # LogisticRegression.transform() will only use the 'features' column.\n",
    "  predictions = trainedModel.transform(testData)\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "def evaluateMLModel(predictions):\n",
    "  # Evaluate model\n",
    "  evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "  return evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method takes dictionary of SparkDataframes and performes on it Regression model\n",
    "#It returns Pandas DataFrame with statistics\n",
    "def calculateLogisticRegressionAccuracy(inputDict):\n",
    "  print\"key | count | FN | TN | TP | TN | ROC_eval \"\n",
    "  for key, value in inputDict.iteritems():\n",
    "      #drop all nulls\n",
    "      stock = value.na.drop()\n",
    "      cols = dataset.columns\n",
    "      #transform data to ML freindly format\n",
    "      MLinput = buildInputMLObject(stock)\n",
    "      #split data into training and test sets\n",
    "      (trainingData, testData) = MLinput.randomSplit([0.8, 0.2], seed = 100)\n",
    "      #train the model \n",
    "      model = trainMLModel(MLinput, trainingData)\n",
    "      #test model\n",
    "      predictions = testMLModel(model, testData)\n",
    "      #evaluate results\n",
    "      testCount = predictions.count()\n",
    "      FP = predictions.where(\"prediction = 0 AND label=1\").count() #FN\n",
    "      FN = predictions.where(\"prediction = 1 AND label=0\").count() #TN\n",
    "      TP = predictions.where(\"prediction = 1 AND label=1\").count() #TP\n",
    "      TN = predictions.where(\"prediction = 0 AND label=0\").count() #TN\n",
    "      result = evaluateMLModel(predictions)\n",
    "      print str(key) +\" | \"+str(testCount)+\" | \"+str(FP)+\" | \"+str(FN)+\" | \"+str(TP)+\" | \"+str(TN)+\" | \"+str(result)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculateLogisticRegressionAccuracy(swissCompDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculateLogisticRegressionAccuracy(nonSwissCompDict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "Masters-ML-Classification",
  "notebookId": 4127206637364224
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
