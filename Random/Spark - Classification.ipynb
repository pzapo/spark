{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Spark libs\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark import SQLContext\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Others\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Graphs libs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "OBV_i = True\n",
    "CCI_i = True\n",
    "RSI_i = True\n",
    "MACD_i = True\n",
    "ManualSplit = False\n",
    "Date_Convert = False\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "DT = False\n",
    "RT = False\n",
    "GBT = True\n",
    "\n",
    "\n",
    "RANDOM_SEED = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def RSI(dataframe, window_length, avg_type, column='Close'):\n",
    "    data = dataframe.toPandas()\n",
    "    # Get just the close\n",
    "    close = data['Close']\n",
    "    # Get the difference in price from previous step\n",
    "    delta = close.diff()\n",
    "    # Get rid of the first row, which is NaN since it did not have a previous\n",
    "    # row to calculate the differences\n",
    "    # Make the positive gains (up) and negative gains (down) Series\n",
    "    up, down = delta.copy(), delta.copy()\n",
    "    up[up < 0] = 0\n",
    "    down[down > 0] = 0\n",
    "    if avg_type == \"EWMA\":\n",
    "        roll_up = up.ewm(span=window_length, min_periods=window_length).mean()\n",
    "        roll_down = down.abs().ewm(\n",
    "            span=window_length, min_periods=window_length).mean()\n",
    "    elif avg_type == \"SMA\":\n",
    "        roll_up = pd.rolling_mean(up, window_length)\n",
    "        roll_down = pd.rolling_mean(down.abs(), window_length)\n",
    "    RS = roll_up / roll_down\n",
    "    RSI = 100.0 - (100.0 / (1.0 + RS))\n",
    "    RSI = pd.DataFrame({'RSI': RSI})\n",
    "    data = data.join(RSI)\n",
    "    result_df = spark.createDataFrame(data)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/home/zapo/Documents/Spark/Random/Datasets/Orlen.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.csv.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/zapo/Documents/Spark/Random/Datasets/Orlen.csv;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:360)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:348)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:348)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:533)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-271e14890fd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpath_to_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./Datasets/Orlen.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfresh_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m temporary_df = fresh_df.select(\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/home/zapo/Documents/Spark/Random/Datasets/Orlen.csv;'"
     ]
    }
   ],
   "source": [
    "# Spark context simple configuration\n",
    "spark = SparkSession.builder.config(conf=SparkConf()).getOrCreate()\n",
    "\n",
    "#Path to file with CSV\n",
    "path_to_csv = \"./Datasets/Orlen.csv\"\n",
    "\n",
    "fresh_df = spark.read.csv(path_to_csv, header=True, inferSchema=True)\n",
    "\n",
    "temporary_df = fresh_df.select(\n",
    "    fresh_df[\"Date\"].cast(\"Date\"), fresh_df[\"Open\"].cast(\"float\"),\n",
    "    fresh_df[\"High\"].cast(\"float\"), fresh_df[\"Volume\"].cast(\"int\"),\n",
    "    fresh_df[\"Low\"].cast(\"float\"), fresh_df[\"Close\"].cast(\"float\"))\n",
    "\n",
    "df_cleared = temporary_df.filter(temporary_df.Open.isNotNull())\n",
    "\n",
    "if DEBUG:\n",
    "    df_cleared.show()\n",
    "\n",
    "# Creating new column with shifted Close price by 1 day\n",
    "df_lag = df_cleared.withColumn('prev_day_price',\n",
    "                               func.lag(df_cleared['Close']).over(\n",
    "                                   Window.orderBy(\"Date\")))\n",
    "\n",
    "# Daily return calculation\n",
    "df_daily_return = df_lag.withColumn(\n",
    "    'Daily return', (df_lag['Close'] - df_lag['prev_day_price']))\n",
    "\n",
    "# Profit label calculation\n",
    "# 1 if stock risen up, 0 is it went down\n",
    "df_profit = df_daily_return.withColumn(\n",
    "    'Profit', (F.when(df_daily_return[\"Daily return\"] < 0, 0).otherwise(1)))\n",
    "\n",
    "df_shifted_profit = df_profit.withColumn(\n",
    "    'Profit',\n",
    "    func.lag(df_profit['Profit'], count=-1).over(Window.orderBy(\"Date\")))\n",
    "\n",
    "final_df = df_shifted_profit.filter(\n",
    "    df_shifted_profit[\"Daily return\"].isNotNull())\n",
    "\n",
    "final_df = final_df.drop(\"prev_day_price\").withColumnRenamed(\n",
    "    existing=\"Daily return\", new=\"Daily return\")\n",
    "\n",
    "# Removing redudant columns\n",
    "final_df = final_df.drop(\"Daily return\")\n",
    "final_df = final_df.drop(\"prev_day_price\")\n",
    "\n",
    "final_df = final_df.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "if DEBUG:\n",
    "    final_df.show()\n",
    "\n",
    "#Conversion to desired typesf\n",
    "converted_df = final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Commodity Channel Index\n",
    "def CCI(spark_df, ndays):\n",
    "    data = spark_df.toPandas()\n",
    "    TP = (data['High'] + data['Low'] + data['Close']) / 3\n",
    "    CCI = pd.Series(\n",
    "        (TP - pd.rolling_mean(TP, ndays)) /\n",
    "        (0.015 * pd.rolling_std(TP, ndays)),\n",
    "        name='CCI')\n",
    "    data = data.join(CCI)\n",
    "    result_df = spark.createDataFrame(data)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Moving average convergence divergence\n",
    "def MACD(dataframe, nfast=12, nslow=24, column='Close'):\n",
    "    data = dataframe.toPandas()\n",
    "    # Get just the close\n",
    "    price = data[column]\n",
    "    # Get the difference in price from previous step\n",
    "    emaslow = pd.ewma(price, span=nslow, min_periods=1)\n",
    "    emafast = pd.ewma(price, span=nfast, min_periods=1)\n",
    "    #     MACD = pd.DataFrame({'MACD': emafast-emaslow, 'emaSlw': emaslow, 'emaFst': emafast})\n",
    "    MACD = pd.DataFrame({'MACD': emafast - emaslow})\n",
    "    data = data.join(MACD)\n",
    "    result_df = spark.createDataFrame(data)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "#Columns without Date\n",
    "# converted_df = converted_df.select(\n",
    "#     [col(c).cast('float') for c in converted_df.columns if c. not in {'Date'}])\n",
    "# Date column!\n",
    "df_date = converted_df.select(converted_df.Date)\n",
    "\n",
    "#Convert date to splitted format\n",
    "if Date_Convert:\n",
    "    split_col = pyspark.sql.functions.split(converted_df['Date'], '-')\n",
    "    converted_df = converted_df.withColumn('Year',\n",
    "                                           split_col.getItem(0).cast('int'))\n",
    "    converted_df = converted_df.withColumn('Month',\n",
    "                                           split_col.getItem(1).cast('int'))\n",
    "    converted_df = converted_df.withColumn('Day',\n",
    "                                           split_col.getItem(2).cast('int'))\n",
    "    if DEBUG:\n",
    "        converted_df.show()\n",
    "\n",
    "converted_df = converted_df.drop(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "if MACD_i:\n",
    "    converted_df = MACD(converted_df)\n",
    "    converted_df = converted_df.select(\n",
    "        [col(c).cast('float') for c in converted_df.columns])\n",
    "    if DEBUG:\n",
    "        converted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "if CCI_i:\n",
    "    converted_df = CCI(converted_df, 14)\n",
    "    converted_df = converted_df.select(\n",
    "        [col(c).cast('float') for c in converted_df.columns])\n",
    "    if DEBUG:\n",
    "        converted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# OBV indicator calculation\n",
    "if OBV_i:\n",
    "    temp_df = converted_df.toPandas()\n",
    "    df_obv = spark.createDataFrame(\n",
    "        temp_df.assign(OBV=(temp_df.Volume * (\n",
    "            ~temp_df.Close.diff().le(0) * 2 - 1)).cumsum()))\n",
    "    converted_df = df_obv.select(\n",
    "        [col(c).cast('float') for c in df_obv.columns])\n",
    "    if DEBUG:\n",
    "        converted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "#RSI indicator calculaction\n",
    "if RSI_i:\n",
    "    converted_df = RSI(converted_df, 3, 'SMA')\n",
    "    if DEBUG:\n",
    "        converted_df.show()\n",
    "        print(converted_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    " if DEBUG:\n",
    "    df_to_plot_dt = converted_df.select('Close').toPandas()\n",
    "    plt_dt.figure(figsize=(14, 14))\n",
    "    plt_dt.plot(df_to_plot_dt)\n",
    "    plt_dt.legend(df_to_plot_dt.columns)\n",
    "    plt_dt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "if CCI_i:\n",
    "    converted_df = converted_df.filter(converted_df.CCI != \"NaN\")\n",
    "\n",
    "converted_df = converted_df.filter(converted_df.Profit != \"NaN\")\n",
    "\n",
    "converted_df = converted_df.sort(converted_df.id.asc())\n",
    "\n",
    "# Manual split for training and validating data\n",
    "if ManualSplit:\n",
    "    dfp = converted_df.toPandas()\n",
    "\n",
    "    dfp = np.array_split(dfp, 10)\n",
    "\n",
    "    p0 = spark.createDataFrame(data=dfp[0])\n",
    "    p1 = spark.createDataFrame(data=dfp[1])\n",
    "    p2 = spark.createDataFrame(data=dfp[2])\n",
    "    p3 = spark.createDataFrame(data=dfp[3])\n",
    "    p4 = spark.createDataFrame(data=dfp[4])\n",
    "    p5 = spark.createDataFrame(data=dfp[5])\n",
    "    p6 = spark.createDataFrame(data=dfp[6])\n",
    "    p7 = spark.createDataFrame(data=dfp[7])\n",
    "    p8 = spark.createDataFrame(data=dfp[8])\n",
    "    p9 = spark.createDataFrame(data=dfp[9])\n",
    "\n",
    "    p_final = p0.union(p1).union(p2).union(p3).union(p4).union(p5).union(\n",
    "        p6).union(p7).union(p8)\n",
    "    train = p_final\n",
    "    test = p9\n",
    "    #     test = p9.head(10)\n",
    "    #     test = spark.createDataFrame(test)\n",
    "else:\n",
    "    train, test = converted_df.randomSplit([0.9, 0.1],seed=RANDOM_SEED)\n",
    "\n",
    "print(\"We have %d training examples and %d test examples.\" % (train.count(),\n",
    "                                                              test.count()))\n",
    "test = test.select(\n",
    "        [col(c).cast('float') for c in test.columns])\n",
    "\n",
    "train = train.select(\n",
    "        [col(c).cast('float') for c in train.columns])\n",
    "\n",
    "test = test.sort(test.id.asc())\n",
    "\n",
    "train = train.sort(train.id.asc())\n",
    "\n",
    "\n",
    "train.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "\n",
    "featuresCols = converted_df.columns\n",
    "featuresCols.remove('Profit')\n",
    "featuresCols.remove('id')\n",
    "\n",
    "print(featuresCols)\n",
    "\n",
    "# Vector Assembler\n",
    "# This concatenates all feature columns into a single feature vector in a new column \"rawFeatures\".\n",
    "# Used for assembling features into a vector.\n",
    "# We will pass all the columns that we are going to use for the prediction to the VectorAssembler and\n",
    "# it will create a new vector column.\n",
    "vectorAssembler_rt = VectorAssembler(\n",
    "    inputCols=featuresCols, outputCol=\"rawFeatures\")\n",
    "\n",
    "# VectorIndexer:\n",
    "# is used to index categorical predictors in a featuresCol column.\n",
    "# Remember that featuresCol is a single column consisting of vectors (refer to featuresCol and labelCol).\n",
    "# Each row is a vector which contains values from each predictors.\n",
    "\n",
    "featureIndexer_rt = VectorIndexer(\n",
    "    inputCol=\"rawFeatures\",\n",
    "    outputCol=\"features\",\n",
    "    maxCategories=len(featuresCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "if RT:\n",
    "    # Algorithm\n",
    "    from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "    rt = RandomForestClassifier(labelCol='Profit', featuresCol=\"features\")\n",
    "\n",
    "    # Define a grid of hyperparameters to test:\n",
    "    #  - maxDepth: max depth of each decision tree in the ensemble\n",
    "    # In practice, to get the highest accuracy, you would likely want to try deeper trees (10 or higher)\n",
    "    # and more trees in the ensemble (>100).\n",
    "\n",
    "    max_Depth_Range = list(range(10, 20))\n",
    "    maxTrees = list(range(50, 100))\n",
    "\n",
    "    paramGrid_rt = ParamGridBuilder() \\\n",
    "        .addGrid(rt.maxDepth, max_Depth_Range) \\\n",
    "        .addGrid(rt.numTrees, maxTrees) \\\n",
    "        .addGrid(rt.maxMemoryInMB, [1500] ).build()\n",
    "\n",
    "    # We define an evaluation metric. This tells CrossValidator how well we are doing by comparing the true\n",
    "    # labels with predictions.\n",
    "\n",
    "    evaluator_rt = BinaryClassificationEvaluator(\n",
    "        labelCol=rt.getLabelCol(),\n",
    "        metricName='areaUnderROC',\n",
    "        rawPredictionCol=rt.getRawPredictionCol())\n",
    "\n",
    "    evaluator_rt_PR = BinaryClassificationEvaluator(\n",
    "        labelCol=rt.getLabelCol(),\n",
    "        metricName='areaUnderPR',\n",
    "        rawPredictionCol=rt.getRawPredictionCol())\n",
    "\n",
    "    # Declare the CrossValidator, which runs model tuning for us.\n",
    "    cv_rt = CrossValidator(\n",
    "        estimator=rt,\n",
    "        evaluator=evaluator_rt,\n",
    "        estimatorParamMaps=paramGrid_rt,\n",
    "        numFolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "if DT:\n",
    "    # Algorithm\n",
    "    from pyspark.ml.classification import DecisionTreeClassifier\n",
    "    \n",
    "    rt = DecisionTreeClassifier(labelCol='Profit', featuresCol=\"features\")\n",
    "\n",
    "    # Define a grid of hyperparameters to test:\n",
    "    #  - maxDepth: max depth of each decision tree in the ensemble\n",
    "    # In practice, to get the highest accuracy, you would likely want to try deeper trees (10 or higher)\n",
    "    # and more trees in the ensemble (>100).\n",
    "\n",
    "    max_Depth_Range = list(range(12, 20))\n",
    "    min_InstancesPerNode = list(range(1, 5))\n",
    "\n",
    "    paramGrid_rt = ParamGridBuilder() \\\n",
    "        .addGrid(rt.maxDepth, max_Depth_Range) \\\n",
    "        .addGrid(rt.maxBins, max_bins) \\\n",
    "        .addGrid(rt.minInstancesPerNode, min_InstancesPerNode) \\\n",
    "        .addGrid(rt.maxMemoryInMB, [1500] ).build()\n",
    "\n",
    "    # We define an evaluation metric. This tells CrossValidator how well we are doing by comparing the true\n",
    "    # labels with predictions.\n",
    "\n",
    "    evaluator_rt = BinaryClassificationEvaluator(\n",
    "        labelCol=rt.getLabelCol(),\n",
    "        metricName='areaUnderROC',\n",
    "        rawPredictionCol=rt.getRawPredictionCol())\n",
    "\n",
    "    evaluator_rt_PR = BinaryClassificationEvaluator(\n",
    "        labelCol=rt.getLabelCol(),\n",
    "        metricName='areaUnderPR',\n",
    "        rawPredictionCol=rt.getRawPredictionCol())\n",
    "\n",
    "    # Declare the CrossValidator, which runs model tuning for us.\n",
    "    cv_rt = CrossValidator(\n",
    "        estimator=rt,\n",
    "        evaluator=evaluator_rt,\n",
    "        estimatorParamMaps=paramGrid_rt,\n",
    "        numFolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GBT:\n",
    "    # Algorithm\n",
    "    from pyspark.ml.classification import GBTClassifier\n",
    "    \n",
    "    rt = GBTClassifier(labelCol='Profit', featuresCol=\"features\")\n",
    "\n",
    "    # Define a grid of hyperparameters to test:\n",
    "    #  - maxDepth: max depth of each decision tree in the ensemble\n",
    "    # In practice, to get the highest accuracy, you would likely want to try deeper trees (10 or higher)\n",
    "    # and more trees in the ensemble (>100).\n",
    "\n",
    "    max_Depth_Range = list(range(5, 12))\n",
    "    min_InstancesPerNode = list(range(1, 5))\n",
    "    \n",
    "    paramGrid_rt = ParamGridBuilder() \\\n",
    "        .addGrid(rt.maxDepth, max_Depth_Range) \\\n",
    "        .addGrid(rt.minInstancesPerNode, min_InstancesPerNode) \\\n",
    "        .addGrid(rt.maxMemoryInMB, [1000] ).build()\n",
    "\n",
    "    # We define an evaluation metric. This tells CrossValidator how well we are doing by comparing the true\n",
    "    # labels with predictions.\n",
    "\n",
    "    evaluator_rt = BinaryClassificationEvaluator(\n",
    "        labelCol=rt.getLabelCol(),\n",
    "        metricName='areaUnderROC',\n",
    "        PredictionCol=rt.getPredictionCol())\n",
    "\n",
    "    evaluator_rt_PR = BinaryClassificationEvaluator(\n",
    "        labelCol=rt.getLabelCol(),\n",
    "        metricName='areaUnderPR',\n",
    "        PredictionCol=rt.getPredictionCol())\n",
    "\n",
    "    # Declare the CrossValidator, which runs model tuning for us.\n",
    "    cv_rt = CrossValidator(\n",
    "        estimator=rt,\n",
    "        evaluator=evaluator_rt,\n",
    "        estimatorParamMaps=paramGrid_rt,\n",
    "        numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "RandomForestClassificationModel.getMaxDepth = (\n",
    "    lambda self: self._java_obj.getMaxDepth())\n",
    "\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "\n",
    "DecisionTreeClassificationModel.getMaxDepth = (\n",
    "    lambda self: self._java_obj.getMaxDepth())\n",
    "\n",
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "\n",
    "GBTClassificationModel.getMaxDepth = (\n",
    "    lambda self: self._java_obj.getMaxDepth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# if DEBUG:\n",
    "#     train.repartition(1).write.csv(\"PreProcessedSets/TrainSet_\" + str(time.mktime(datetime.datetime.today().timetuple())) + \"_.csv\", header = 'True')\n",
    "#     test.repartition(1).write.csv(\"PreProcessedSets/TestSet_\" + str(time.mktime(datetime.datetime.today().timetuple())) + \"_.csv\", header = 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Creating Final pipeline object\n",
    "pipeline_rt = Pipeline(stages=[vectorAssembler_rt, featureIndexer_rt, cv_rt])\n",
    "\n",
    "# FITTING!\n",
    "pipelineModel_rt = pipeline_rt.fit(train)\n",
    "\n",
    "# Getting the Best Model\n",
    "best_classifier = pipelineModel_rt.stages[-1].bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "print('Features importances' + str(best_classifier.featureImportances))\n",
    "\n",
    "# Max depth\n",
    "print(\"Maximal depth is \" + str(best_classifier.getMaxDepth()))\n",
    "max_depth = best_classifier.getMaxDepth()\n",
    "\n",
    "# Best number of Trees\n",
    "print(\"Best number of trees is \" + str(best_classifier.getNumTrees))\n",
    "\n",
    "# Min instances\n",
    "print(\"Minimal instances per node is \" + str(best_classifier.getMinInstancesPerNode()))\n",
    "min_instancesPerNode = best_classifier.getMinInstancesPerNode()\n",
    "\n",
    "# Making Predictions!\n",
    "predictions_rt = pipelineModel_rt.transform(test)\n",
    "\n",
    "# Calculating metrics\n",
    "AreaUnderROC = evaluator_rt.evaluate(predictions_rt)\n",
    "print(\"AreaUnderROC on our test set: %g\" % AreaUnderROC)\n",
    "\n",
    "# Calculating metrics\n",
    "AreaUnderPR = evaluator_rt_PR.evaluate(predictions_rt)\n",
    "print(\"AreaUnderPR on our test set: %g\" % AreaUnderPR)\n",
    "\n",
    "#evaluate results\n",
    "testCount = predictions_rt.count()\n",
    "\n",
    "FP = predictions_rt.where(\"prediction = 0 AND Profit=1\").count() #FN\n",
    "FN = predictions_rt.where(\"prediction = 1 AND Profit=0\").count() #TN\n",
    "TP = predictions_rt.where(\"prediction = 1 AND Profit=1\").count() #TP\n",
    "TN = predictions_rt.where(\"prediction = 0 AND Profit=0\").count() #TN\n",
    "\n",
    "print(\"Test count | FN | TN | TP | TN\")\n",
    "print(str(testCount)+\" | \"+str(FP)+\" | \"+str(FN)+\" | \"+str(TP)+\" | \"+str(TN)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model = pipelineModel_rt\n",
    "from random import *\n",
    "\n",
    "for i in range(10):\n",
    "    new_train, new_test = converted_df.randomSplit([0.9, 0.1], seed=i+1)\n",
    "    new_test = new_test.sort(new_test.id.asc())\n",
    "    \n",
    "    predictions = pipelineModel_rt.transform(new_test)\n",
    "    \n",
    "    # Calculating metrics\n",
    "    AreaUnderROC = evaluator_rt.evaluate(predictions)\n",
    "    print(\"AreaUnderROC on our test set: %g\" % AreaUnderROC)\n",
    "\n",
    "    # Calculating metrics\n",
    "    AreaUnderPR = evaluator_rt_PR.evaluate(predictions)\n",
    "    print(\"AreaUnderPR on our test set: %g\" % AreaUnderPR)\n",
    "\n",
    "    #evaluate results\n",
    "    testCount = predictions_rt.count()\n",
    "\n",
    "    FP = predictions_rt.where(\"prediction = 0 AND Profit=1\").count() #FN\n",
    "    FN = predictions_rt.where(\"prediction = 1 AND Profit=0\").count() #TN\n",
    "    TP = predictions_rt.where(\"prediction = 1 AND Profit=1\").count() #TP\n",
    "    TN = predictions_rt.where(\"prediction = 0 AND Profit=0\").count() #TN\n",
    "\n",
    "    print(\"Test count | FN | TN | TP | TN\")\n",
    "    print(str(testCount)+\" | \"+str(FP)+\" | \"+str(FN)+\" | \"+str(TP)+\" | \"+str(TN))\n",
    "    \n",
    "    print(\"####################################################\\n\\n\")\n",
    "    \n",
    "    \n",
    "predictions_rt.repartition(1).write.csv(\"RT_Final.csv\", header = 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "test = predictions_rt.toPandas()\n",
    "csv_buffer = StringIO()\n",
    "test.to_csv(csv_buffer)\n",
    "\n",
    "# s3_resource = boto3.resource('s3')\n",
    "\n",
    "# s3_resource.Object('logs102', 'RT_Final.csv').put(Body=csv_buffer.getvalue())\n",
    "\n",
    "# if DEBUG != True:\n",
    "#     df_to_plot_rt = predictions_rt.select('prediction', 'Profit')\n",
    "#     print(df_to_plot_rt)\n",
    "#     df_to_plot_rt = df_to_plot_rt.toPandas()\n",
    "#     plt_dt.figure(figsize=(14, 14))\n",
    "#     plt_dt.plot(df_to_plot_rt)\n",
    "#     plt_dt.legend(df_to_plot_rt.columns)\n",
    "#     plt_dt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
