{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark libs\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark import SQLContext\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Others\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Graphs libs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt_dt\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "OBV_i = True\n",
    "CCI_i = True\n",
    "RSI_i = True\n",
    "MACD_i = True\n",
    "ManualSplit = False\n",
    "Date_Convert = False\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "DT = True\n",
    "RT = False\n",
    "GBT = False\n",
    "\n",
    "\n",
    "RANDOM_SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def RSI(dataframe, window_length, avg_type, column='Close'):\n",
    "    data = dataframe.toPandas()\n",
    "    # Get just the close\n",
    "    close = data['Close']\n",
    "    # Get the difference in price from previous step\n",
    "    delta = close.diff()\n",
    "    # Get rid of the first row, which is NaN since it did not have a previous\n",
    "    # row to calculate the differences\n",
    "    # Make the positive gains (up) and negative gains (down) Series\n",
    "    up, down = delta.copy(), delta.copy()\n",
    "    up[up < 0] = 0\n",
    "    down[down > 0] = 0\n",
    "    if avg_type == \"EWMA\":\n",
    "        roll_up = up.ewm(span=window_length, min_periods=window_length).mean()\n",
    "        roll_down = down.abs().ewm(\n",
    "            span=window_length, min_periods=window_length).mean()\n",
    "    elif avg_type == \"SMA\":\n",
    "        roll_up = pd.rolling_mean(up, window_length)\n",
    "        roll_down = pd.rolling_mean(down.abs(), window_length)\n",
    "    RS = roll_up / roll_down\n",
    "    RSI = 100.0 - (100.0 / (1.0 + RS))\n",
    "    RSI = pd.DataFrame({'RSI': RSI})\n",
    "    data = data.join(RSI)\n",
    "    result_df = spark.createDataFrame(data)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commodity Channel Index\n",
    "def CCI(spark_df, ndays):\n",
    "    data = spark_df.toPandas()\n",
    "    TP = (data['High'] + data['Low'] + data['Close']) / 3\n",
    "    CCI = pd.Series(\n",
    "        (TP - pd.rolling_mean(TP, ndays)) /\n",
    "        (0.015 * pd.rolling_std(TP, ndays)),\n",
    "        name='CCI')\n",
    "    data = data.join(CCI)\n",
    "    result_df = spark.createDataFrame(data)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving average convergence divergence\n",
    "def MACD(dataframe, nfast=12, nslow=24, column='Close'):\n",
    "    data = dataframe.toPandas()\n",
    "    # Get just the close\n",
    "    price = data[column]\n",
    "    # Get the difference in price from previous step\n",
    "    emaslow = pd.ewma(price, span=nslow, min_periods=1)\n",
    "    emafast = pd.ewma(price, span=nfast, min_periods=1)\n",
    "    #     MACD = pd.DataFrame({'MACD': emafast-emaslow, 'emaSlw': emaslow, 'emaFst': emafast})\n",
    "    MACD = pd.DataFrame({'MACD': emafast - emaslow})\n",
    "    data = data.join(MACD)\n",
    "    result_df = spark.createDataFrame(data)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+------+-----+-----+\n",
      "|      Date| Open| High|Volume|  Low|Close|\n",
      "+----------+-----+-----+------+-----+-----+\n",
      "|2012-05-25|7.706|7.706|     0|7.706|7.706|\n",
      "|2012-05-29|7.802|7.809|     0|7.802|7.809|\n",
      "|2012-05-30| 7.95| 7.95|    10|7.782|7.782|\n",
      "|2012-05-31|7.688|7.688|     0|7.688|7.688|\n",
      "|2012-06-01|7.526|7.526|   300|7.459|7.459|\n",
      "|2012-06-04|7.328|7.328|     0|7.328|7.328|\n",
      "|2012-06-05|7.514|7.514|     0|7.514|7.514|\n",
      "|2012-06-06|7.317|7.317|     0|7.317|7.317|\n",
      "|2012-06-08|7.502|7.569|     0|7.502|7.569|\n",
      "|2012-06-11|7.701|7.701|     0|7.701|7.701|\n",
      "|2012-06-12|7.671|7.671|     0|7.671|7.671|\n",
      "|2012-06-13|7.935|7.935|     0|7.935|7.935|\n",
      "|2012-06-14|7.978|7.978|     0|7.978|7.978|\n",
      "|2012-06-15|8.089|8.089|     0|8.089|8.089|\n",
      "|2012-06-18|8.337|8.337|     0|8.337|8.337|\n",
      "|2012-06-19|8.223|8.223|     0|8.223|8.223|\n",
      "|2012-06-20|8.587|8.824|   370|8.587|8.824|\n",
      "|2012-06-21|8.634|8.634|   350|8.616|8.616|\n",
      "|2012-06-22|8.404|8.457|  1250|8.404|8.441|\n",
      "|2012-06-25|8.402|8.402|     0|8.402|8.402|\n",
      "+----------+-----+-----+------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+-----+-----+------+-----+-----+------+\n",
      "|      Date| Open| High|Volume|  Low|Close|Profit|\n",
      "+----------+-----+-----+------+-----+-----+------+\n",
      "|2012-05-29|7.802|7.809|     0|7.802|7.809|     0|\n",
      "|2012-05-30| 7.95| 7.95|    10|7.782|7.782|     0|\n",
      "|2012-05-31|7.688|7.688|     0|7.688|7.688|     0|\n",
      "|2012-06-01|7.526|7.526|   300|7.459|7.459|     0|\n",
      "|2012-06-04|7.328|7.328|     0|7.328|7.328|     1|\n",
      "|2012-06-05|7.514|7.514|     0|7.514|7.514|     0|\n",
      "|2012-06-06|7.317|7.317|     0|7.317|7.317|     1|\n",
      "|2012-06-08|7.502|7.569|     0|7.502|7.569|     1|\n",
      "|2012-06-11|7.701|7.701|     0|7.701|7.701|     0|\n",
      "|2012-06-12|7.671|7.671|     0|7.671|7.671|     1|\n",
      "|2012-06-13|7.935|7.935|     0|7.935|7.935|     1|\n",
      "|2012-06-14|7.978|7.978|     0|7.978|7.978|     1|\n",
      "|2012-06-15|8.089|8.089|     0|8.089|8.089|     1|\n",
      "|2012-06-18|8.337|8.337|     0|8.337|8.337|     0|\n",
      "|2012-06-19|8.223|8.223|     0|8.223|8.223|     1|\n",
      "|2012-06-20|8.587|8.824|   370|8.587|8.824|     0|\n",
      "|2012-06-21|8.634|8.634|   350|8.616|8.616|     0|\n",
      "|2012-06-22|8.404|8.457|  1250|8.404|8.441|     0|\n",
      "|2012-06-25|8.402|8.402|     0|8.402|8.402|     0|\n",
      "|2012-06-26|8.344|8.344|     0|8.344|8.344|     1|\n",
      "+----------+-----+-----+------+-----+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark context simple configuration\n",
    "spark = SparkSession.builder.config(conf=SparkConf()).getOrCreate()\n",
    "\n",
    "#Path to file with CSV\n",
    "path_to_csv = \"s3://stocksets100/Orlen.csv\"\n",
    "\n",
    "fresh_df = spark.read.csv(path_to_csv, header=True, inferSchema=True)\n",
    "\n",
    "temporary_df = fresh_df.select(\n",
    "    fresh_df[\"Date\"].cast(\"Date\"), fresh_df[\"Open\"].cast(\"float\"),\n",
    "    fresh_df[\"High\"].cast(\"float\"), fresh_df[\"Volume\"].cast(\"int\"),\n",
    "    fresh_df[\"Low\"].cast(\"float\"), fresh_df[\"Close\"].cast(\"float\"))\n",
    "\n",
    "df_cleared = temporary_df.filter(temporary_df.Open.isNotNull())\n",
    "\n",
    "if DEBUG:\n",
    "    df_cleared.show()\n",
    "\n",
    "# Creating new column with shifted Close price by 1 day\n",
    "df_lag = df_cleared.withColumn('prev_day_price',\n",
    "                               func.lag(df_cleared['Close']).over(\n",
    "                                   Window.orderBy(\"Date\")))\n",
    "\n",
    "# Daily return calculation\n",
    "df_daily_return = df_lag.withColumn(\n",
    "    'Daily return', (df_lag['Close'] - df_lag['prev_day_price']))\n",
    "\n",
    "# Profit label calculation\n",
    "# 1 if stock risen up, 0 is it went down\n",
    "df_profit = df_daily_return.withColumn(\n",
    "    'Profit', (F.when(df_daily_return[\"Daily return\"] < 0, 0).otherwise(1)))\n",
    "\n",
    "df_shifted_profit = df_profit.withColumn(\n",
    "    'Profit',\n",
    "    func.lag(df_profit['Profit'], count=-1).over(Window.orderBy(\"Date\")))\n",
    "\n",
    "final_df = df_shifted_profit.filter(\n",
    "    df_shifted_profit[\"Daily return\"].isNotNull())\n",
    "\n",
    "final_df = final_df.drop(\"prev_day_price\").withColumnRenamed(\n",
    "    existing=\"Daily return\", new=\"Daily return\")\n",
    "\n",
    "# Removing redudant columns\n",
    "final_df = final_df.drop(\"Daily return\")\n",
    "final_df = final_df.drop(\"prev_day_price\")\n",
    "\n",
    "# final_df = final_df.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "if DEBUG:\n",
    "    final_df.show()\n",
    "\n",
    "#Conversion to desired typesf\n",
    "converted_df = final_df.select(\"*\").withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "#Columns without Date\n",
    "# converted_df = converted_df.select(\n",
    "#     [col(c).cast('float') for c in converted_df.columns if c. not in {'Date'}])\n",
    "# Date column!\n",
    "df_date = converted_df.select(converted_df.Date)\n",
    "\n",
    "#Convert date to splitted format\n",
    "if Date_Convert:\n",
    "    split_col = pyspark.sql.functions.split(converted_df['Date'], '-')\n",
    "    converted_df = converted_df.withColumn('Year',\n",
    "                                           split_col.getItem(0).cast('int'))\n",
    "    converted_df = converted_df.withColumn('Month',\n",
    "                                           split_col.getItem(1).cast('int'))\n",
    "    converted_df = converted_df.withColumn('Day',\n",
    "                                           split_col.getItem(2).cast('int'))\n",
    "    if DEBUG:\n",
    "        converted_df.show()\n",
    "\n",
    "converted_df = converted_df.drop(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: pd.ewm_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.ewm(span=24,min_periods=1,adjust=True,ignore_na=False).mean()\n",
      "  import sys\n",
      "/home/hadoop/conda/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: pd.ewm_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.ewm(span=12,min_periods=1,adjust=True,ignore_na=False).mean()\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+-----+-----+------+----+-------------+\n",
      "| Open| High|Volume|  Low|Close|Profit|  id|         MACD|\n",
      "+-----+-----+------+-----+-----+------+----+-------------+\n",
      "|7.802|7.809|   0.0|7.802|7.809|   0.0| 0.0|          0.0|\n",
      "| 7.95| 7.95|  10.0|7.782|7.782|   0.0| 1.0|  -5.62499E-4|\n",
      "|7.688|7.688|   0.0|7.688|7.688|   0.0| 2.0|-0.0034248475|\n",
      "|7.526|7.526| 300.0|7.459|7.459|   0.0| 3.0| -0.012326616|\n",
      "|7.328|7.328|   0.0|7.328|7.328|   1.0| 4.0| -0.021733696|\n",
      "|7.514|7.514|   0.0|7.514|7.514|   0.0| 5.0| -0.019541662|\n",
      "|7.317|7.317|   0.0|7.317|7.317|   1.0| 6.0|  -0.02610725|\n",
      "|7.502|7.569|   0.0|7.502|7.569|   1.0| 7.0| -0.018885486|\n",
      "|7.701|7.701|   0.0|7.701|7.701|   0.0| 8.0|-0.0074942946|\n",
      "|7.671|7.671|   0.0|7.671|7.671|   1.0| 9.0| -7.705355E-4|\n",
      "|7.935|7.935|   0.0|7.935|7.935|   1.0|10.0|  0.017208392|\n",
      "|7.978|7.978|   0.0|7.978|7.978|   1.0|11.0|   0.03229937|\n",
      "|8.089|8.089|   0.0|8.089|8.089|   1.0|12.0|  0.048852794|\n",
      "|8.337|8.337|   0.0|8.337|8.337|   0.0|13.0|   0.07394994|\n",
      "|8.223|8.223|   0.0|8.223|8.223|   1.0|14.0|   0.08548735|\n",
      "|8.587|8.824| 370.0|8.587|8.824|   0.0|15.0|   0.12712103|\n",
      "|8.634|8.634| 350.0|8.616|8.616|   0.0|16.0|   0.14504555|\n",
      "|8.404|8.457|1250.0|8.404|8.441|   0.0|17.0|   0.14652483|\n",
      "|8.402|8.402|   0.0|8.402|8.402|   0.0|18.0|   0.14343652|\n",
      "|8.344|8.344|   0.0|8.344|8.344|   1.0|19.0|   0.13570516|\n",
      "+-----+-----+------+-----+-----+------+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if MACD_i:\n",
    "    converted_df = MACD(converted_df)\n",
    "    converted_df = converted_df.select(\n",
    "        [col(c).cast('float') for c in converted_df.columns])\n",
    "    if DEBUG:\n",
    "        converted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=14,center=False).mean()\n",
      "  \n",
      "/home/hadoop/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: pd.rolling_std is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=14,center=False).std()\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+-----+-----+------+----+-------------+---------+\n",
      "| Open| High|Volume|  Low|Close|Profit|  id|         MACD|      CCI|\n",
      "+-----+-----+------+-----+-----+------+----+-------------+---------+\n",
      "|7.802|7.809|   0.0|7.802|7.809|   0.0| 0.0|          0.0|      NaN|\n",
      "| 7.95| 7.95|  10.0|7.782|7.782|   0.0| 1.0|  -5.62499E-4|      NaN|\n",
      "|7.688|7.688|   0.0|7.688|7.688|   0.0| 2.0|-0.0034248475|      NaN|\n",
      "|7.526|7.526| 300.0|7.459|7.459|   0.0| 3.0| -0.012326616|      NaN|\n",
      "|7.328|7.328|   0.0|7.328|7.328|   1.0| 4.0| -0.021733696|      NaN|\n",
      "|7.514|7.514|   0.0|7.514|7.514|   0.0| 5.0| -0.019541662|      NaN|\n",
      "|7.317|7.317|   0.0|7.317|7.317|   1.0| 6.0|  -0.02610725|      NaN|\n",
      "|7.502|7.569|   0.0|7.502|7.569|   1.0| 7.0| -0.018885486|      NaN|\n",
      "|7.701|7.701|   0.0|7.701|7.701|   0.0| 8.0|-0.0074942946|      NaN|\n",
      "|7.671|7.671|   0.0|7.671|7.671|   1.0| 9.0| -7.705355E-4|      NaN|\n",
      "|7.935|7.935|   0.0|7.935|7.935|   1.0|10.0|  0.017208392|      NaN|\n",
      "|7.978|7.978|   0.0|7.978|7.978|   1.0|11.0|   0.03229937|      NaN|\n",
      "|8.089|8.089|   0.0|8.089|8.089|   1.0|12.0|  0.048852794|      NaN|\n",
      "|8.337|8.337|   0.0|8.337|8.337|   0.0|13.0|   0.07394994|139.01875|\n",
      "|8.223|8.223|   0.0|8.223|8.223|   1.0|14.0|   0.08548735| 96.65555|\n",
      "|8.587|8.824| 370.0|8.587|8.824|   0.0|15.0|   0.12712103|148.12761|\n",
      "|8.634|8.634| 350.0|8.616|8.616|   0.0|16.0|   0.14504555|105.22151|\n",
      "|8.404|8.457|1250.0|8.404|8.441|   0.0|17.0|   0.14652483| 67.58636|\n",
      "|8.402|8.402|   0.0|8.402|8.402|   0.0|18.0|   0.14343652| 54.92698|\n",
      "|8.344|8.344|   0.0|8.344|8.344|   1.0|19.0|   0.13570516| 39.07378|\n",
      "+-----+-----+------+-----+-----+------+----+-------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if CCI_i:\n",
    "    converted_df = CCI(converted_df, 14)\n",
    "    converted_df = converted_df.select(\n",
    "        [col(c).cast('float') for c in converted_df.columns])\n",
    "    if DEBUG:\n",
    "        converted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+-----+-----+------+----+-------------+---------+-------+\n",
      "| Open| High|Volume|  Low|Close|Profit|  id|         MACD|      CCI|    OBV|\n",
      "+-----+-----+------+-----+-----+------+----+-------------+---------+-------+\n",
      "|7.802|7.809|   0.0|7.802|7.809|   0.0| 0.0|          0.0|      NaN|    0.0|\n",
      "| 7.95| 7.95|  10.0|7.782|7.782|   0.0| 1.0|  -5.62499E-4|      NaN|  -10.0|\n",
      "|7.688|7.688|   0.0|7.688|7.688|   0.0| 2.0|-0.0034248475|      NaN|  -10.0|\n",
      "|7.526|7.526| 300.0|7.459|7.459|   0.0| 3.0| -0.012326616|      NaN| -310.0|\n",
      "|7.328|7.328|   0.0|7.328|7.328|   1.0| 4.0| -0.021733696|      NaN| -310.0|\n",
      "|7.514|7.514|   0.0|7.514|7.514|   0.0| 5.0| -0.019541662|      NaN| -310.0|\n",
      "|7.317|7.317|   0.0|7.317|7.317|   1.0| 6.0|  -0.02610725|      NaN| -310.0|\n",
      "|7.502|7.569|   0.0|7.502|7.569|   1.0| 7.0| -0.018885486|      NaN| -310.0|\n",
      "|7.701|7.701|   0.0|7.701|7.701|   0.0| 8.0|-0.0074942946|      NaN| -310.0|\n",
      "|7.671|7.671|   0.0|7.671|7.671|   1.0| 9.0| -7.705355E-4|      NaN| -310.0|\n",
      "|7.935|7.935|   0.0|7.935|7.935|   1.0|10.0|  0.017208392|      NaN| -310.0|\n",
      "|7.978|7.978|   0.0|7.978|7.978|   1.0|11.0|   0.03229937|      NaN| -310.0|\n",
      "|8.089|8.089|   0.0|8.089|8.089|   1.0|12.0|  0.048852794|      NaN| -310.0|\n",
      "|8.337|8.337|   0.0|8.337|8.337|   0.0|13.0|   0.07394994|139.01875| -310.0|\n",
      "|8.223|8.223|   0.0|8.223|8.223|   1.0|14.0|   0.08548735| 96.65555| -310.0|\n",
      "|8.587|8.824| 370.0|8.587|8.824|   0.0|15.0|   0.12712103|148.12761|   60.0|\n",
      "|8.634|8.634| 350.0|8.616|8.616|   0.0|16.0|   0.14504555|105.22151| -290.0|\n",
      "|8.404|8.457|1250.0|8.404|8.441|   0.0|17.0|   0.14652483| 67.58636|-1540.0|\n",
      "|8.402|8.402|   0.0|8.402|8.402|   0.0|18.0|   0.14343652| 54.92698|-1540.0|\n",
      "|8.344|8.344|   0.0|8.344|8.344|   1.0|19.0|   0.13570516| 39.07378|-1540.0|\n",
      "+-----+-----+------+-----+-----+------+----+-------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OBV indicator calculation\n",
    "if OBV_i:\n",
    "    temp_df = converted_df.toPandas()\n",
    "    df_obv = spark.createDataFrame(\n",
    "        temp_df.assign(OBV=(temp_df.Volume * (\n",
    "            ~temp_df.Close.diff().le(0) * 2 - 1)).cumsum()))\n",
    "    converted_df = df_obv.select(\n",
    "        [col(c).cast('float') for c in df_obv.columns])\n",
    "    if DEBUG:\n",
    "        converted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/conda/lib/python3.6/site-packages/ipykernel_launcher.py:18: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=3,center=False).mean()\n",
      "/home/hadoop/conda/lib/python3.6/site-packages/ipykernel_launcher.py:19: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=3,center=False).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+------+------------------+------------------+------+----+--------------------+------------------+-------+-----------------+\n",
      "|             Open|             High|Volume|               Low|             Close|Profit|  id|                MACD|               CCI|    OBV|              RSI|\n",
      "+-----------------+-----------------+------+------------------+------------------+------+----+--------------------+------------------+-------+-----------------+\n",
      "|7.802000045776367|7.809000015258789|   0.0| 7.802000045776367| 7.809000015258789|   0.0| 0.0|                 0.0|               NaN|    0.0|              NaN|\n",
      "|7.949999809265137|7.949999809265137|  10.0|7.7820000648498535|7.7820000648498535|   0.0| 1.0|-5.62498986255377...|               NaN|  -10.0|              NaN|\n",
      "|7.688000202178955|7.688000202178955|   0.0| 7.688000202178955| 7.688000202178955|   0.0| 2.0|-0.00342484749853611|               NaN|  -10.0|              NaN|\n",
      "|7.526000022888184|7.526000022888184| 300.0| 7.459000110626221| 7.459000110626221|   0.0| 3.0|-0.01232661586254...|               NaN| -310.0|              0.0|\n",
      "|7.328000068664551|7.328000068664551|   0.0| 7.328000068664551| 7.328000068664551|   1.0| 4.0|-0.02173369564116001|               NaN| -310.0|              0.0|\n",
      "|7.513999938964844|7.513999938964844|   0.0| 7.513999938964844| 7.513999938964844|   0.0| 5.0| -0.0195416621863842|               NaN| -310.0|34.06591007340322|\n",
      "|7.316999912261963|7.316999912261963|   0.0| 7.316999912261963| 7.316999912261963|   1.0| 6.0|-0.02610724978148...|               NaN| -310.0|36.18674949162102|\n",
      "|7.501999855041504|7.568999767303467|   0.0| 7.501999855041504| 7.568999767303467|   1.0| 7.0|-0.01888548582792282|               NaN| -310.0|  68.976361633442|\n",
      "|7.701000213623047|7.701000213623047|   0.0| 7.701000213623047| 7.701000213623047|   0.0| 8.0|-0.00749429455026...|               NaN| -310.0|66.09295775110263|\n",
      "|7.671000003814697|7.671000003814697|   0.0| 7.671000003814697| 7.671000003814697|   1.0| 9.0|-7.70535494666546...|               NaN| -310.0|92.75358145727705|\n",
      "|7.934999942779541|7.934999942779541|   0.0| 7.934999942779541| 7.934999942779541|   1.0|10.0|0.017208391800522804|               NaN| -310.0|92.95770706568702|\n",
      "|7.978000164031982|7.978000164031982|   0.0| 7.978000164031982| 7.978000164031982|   1.0|11.0| 0.03229936957359314|               NaN| -310.0|91.09787036552287|\n",
      "| 8.08899974822998| 8.08899974822998|   0.0|  8.08899974822998|  8.08899974822998|   1.0|12.0|0.048852793872356415|               NaN| -310.0|            100.0|\n",
      "|8.336999893188477|8.336999893188477|   0.0| 8.336999893188477| 8.336999893188477|   0.0|13.0| 0.07394994050264359| 139.0187530517578| -310.0|            100.0|\n",
      "|8.222999572753906|8.222999572753906|   0.0| 8.222999572753906| 8.222999572753906|   1.0|14.0| 0.08548735082149506| 96.65554809570312| -310.0| 75.8984548663092|\n",
      "|8.586999893188477|8.824000358581543| 370.0| 8.586999893188477| 8.824000358581543|   0.0|15.0| 0.12712103128433228| 148.1276092529297|   60.0|88.16197587593336|\n",
      "|8.633999824523926|8.633999824523926| 350.0| 8.616000175476074| 8.616000175476074|   0.0|16.0| 0.14504554867744446|105.22151184082031| -290.0|65.11375365893224|\n",
      "|8.404000282287598|8.456999778747559|1250.0| 8.404000282287598| 8.440999984741211|   0.0|17.0|  0.1465248316526413| 67.58635711669922|-1540.0|61.07724365187051|\n",
      "|8.402000427246094|8.402000427246094|   0.0| 8.402000427246094| 8.402000427246094|   0.0|18.0|  0.1434365212917328|54.926979064941406|-1540.0|              0.0|\n",
      "|8.343999862670898|8.343999862670898|   0.0| 8.343999862670898| 8.343999862670898|   1.0|19.0|  0.1357051581144333| 39.07378005981445|-1540.0|              0.0|\n",
      "+-----------------+-----------------+------+------------------+------------------+------+----+--------------------+------------------+-------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "1362\n"
     ]
    }
   ],
   "source": [
    "#RSI indicator calculaction\n",
    "if RSI_i:\n",
    "    converted_df = RSI(converted_df, 3, 'SMA')\n",
    "    if DEBUG:\n",
    "        converted_df.show()\n",
    "        print(converted_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    df_to_plot_dt = converted_df.select('Close').toPandas()\n",
    "    plt_dt.figure(figsize=(14, 14))\n",
    "    plt_dt.plot(df_to_plot_dt)\n",
    "    plt_dt.legend(df_to_plot_dt.columns)\n",
    "#     plt_dt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+---+-----+------+---+----+---+---+---+\n",
      "|Open|High|Volume|Low|Close|Profit| id|MACD|CCI|OBV|RSI|\n",
      "+----+----+------+---+-----+------+---+----+---+---+---+\n",
      "+----+----+------+---+-----+------+---+----+---+---+---+\n",
      "\n",
      "+-----------------+-----------------+------+-----------------+-----------------+------+----+-------------------+-------------------+-------+------------------+\n",
      "|             Open|             High|Volume|              Low|            Close|Profit|  id|               MACD|                CCI|    OBV|               RSI|\n",
      "+-----------------+-----------------+------+-----------------+-----------------+------+----+-------------------+-------------------+-------+------------------+\n",
      "|8.336999893188477|8.336999893188477|   0.0|8.336999893188477|8.336999893188477|   0.0|13.0|0.07394994050264359|  139.0187530517578| -310.0|             100.0|\n",
      "|8.222999572753906|8.222999572753906|   0.0|8.222999572753906|8.222999572753906|   1.0|14.0|0.08548735082149506|  96.65554809570312| -310.0|  75.8984548663092|\n",
      "|8.586999893188477|8.824000358581543| 370.0|8.586999893188477|8.824000358581543|   0.0|15.0|0.12712103128433228|  148.1276092529297|   60.0| 88.16197587593336|\n",
      "|8.633999824523926|8.633999824523926| 350.0|8.616000175476074|8.616000175476074|   0.0|16.0|0.14504554867744446| 105.22151184082031| -290.0| 65.11375365893224|\n",
      "|8.404000282287598|8.456999778747559|1250.0|8.404000282287598|8.440999984741211|   0.0|17.0| 0.1465248316526413|  67.58635711669922|-1540.0| 61.07724365187051|\n",
      "|8.402000427246094|8.402000427246094|   0.0|8.402000427246094|8.402000427246094|   0.0|18.0| 0.1434365212917328| 54.926979064941406|-1540.0|               0.0|\n",
      "|8.343999862670898|8.343999862670898|   0.0|8.343999862670898|8.343999862670898|   1.0|19.0| 0.1357051581144333|  39.07378005981445|-1540.0|               0.0|\n",
      "|8.583000183105469|8.583000183105469|   0.0|8.583000183105469|8.583000183105469|   0.0|20.0|0.14280620217323303|  70.22051239013672|-1540.0| 71.13095407042422|\n",
      "|8.470999717712402|8.470999717712402|   0.0|8.470999717712402|8.470999717712402|   1.0|21.0|0.13945703208446503| 43.572330474853516|-1540.0| 58.43509323359814|\n",
      "|8.564000129699707|8.564000129699707|   0.0|8.564000129699707|8.564000129699707|   1.0|22.0|0.14101774990558624|  54.99522399902344|-1540.0| 74.77473800875917|\n",
      "| 8.82800006866455| 8.82800006866455|   0.0| 8.82800006866455| 8.82800006866455|   1.0|23.0|0.15740029513835907|  106.4896011352539|-1540.0|  76.1193453210054|\n",
      "|8.928999900817871|8.928999900817871|   0.0|8.928999900817871|8.928999900817871|   0.0|24.0| 0.1746593713760376| 113.93231201171875|-1540.0|             100.0|\n",
      "|8.902000427246094|8.902000427246094|   0.0|8.902000427246094|8.902000427246094|   1.0|25.0|0.18407641351222992|  96.88768005371094|-1540.0| 93.11236591970145|\n",
      "|9.092000007629395|9.092000007629395|   0.0|9.092000007629395|9.092000007629395|   0.0|26.0|0.20165079832077026| 124.93189239501953|-1540.0| 91.50956976541929|\n",
      "|9.057000160217285|9.057000160217285|   0.0|9.057000160217285|9.057000160217285|   0.0|27.0| 0.2104398012161255|   97.5958480834961|-1540.0| 75.39698758704209|\n",
      "|8.930000305175781|8.930000305175781|   0.0|8.930000305175781|8.930000305175781|   1.0|28.0|0.20610910654067993|  59.03043746948242|-1540.0|53.977263490996975|\n",
      "| 9.01200008392334| 9.01200008392334|   0.0| 9.01200008392334| 9.01200008392334|   1.0|29.0|0.20574679970741272|  72.02796173095703|-1540.0| 33.60653815487079|\n",
      "|9.107000350952148|9.107000350952148|   0.0|9.107000350952148|9.107000350952148|   0.0|30.0|0.20939664542675018|   82.1169662475586|-1540.0|58.223718264437665|\n",
      "|8.986000061035156|8.986000061035156|   0.0|8.986000061035156|8.986000061035156|   0.0|31.0|  0.201339453458786|  45.81468963623047|-1540.0|  59.3959216067794|\n",
      "| 8.77299976348877| 8.77299976348877|   0.0| 8.77299976348877| 8.77299976348877|   1.0|32.0| 0.1778886467218399|-14.703465461730957|-1540.0|22.144540280988792|\n",
      "+-----------------+-----------------+------+-----------------+-----------------+------+----+-------------------+-------------------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "We have 1219 training examples and 129 test examples.\n",
      "+-----+-----+------+-----+-----+------+----+----------+----------+-------+----------+\n",
      "| Open| High|Volume|  Low|Close|Profit|  id|      MACD|       CCI|    OBV|       RSI|\n",
      "+-----+-----+------+-----+-----+------+----+----------+----------+-------+----------+\n",
      "|8.337|8.337|   0.0|8.337|8.337|   0.0|13.0|0.07394994| 139.01875| -310.0|     100.0|\n",
      "|8.223|8.223|   0.0|8.223|8.223|   1.0|14.0|0.08548735|  96.65555| -310.0|  75.89845|\n",
      "|8.587|8.824| 370.0|8.587|8.824|   0.0|15.0|0.12712103| 148.12761|   60.0|  88.16198|\n",
      "|8.634|8.634| 350.0|8.616|8.616|   0.0|16.0|0.14504555| 105.22151| -290.0| 65.113754|\n",
      "|8.402|8.402|   0.0|8.402|8.402|   0.0|18.0|0.14343652|  54.92698|-1540.0|       0.0|\n",
      "|8.344|8.344|   0.0|8.344|8.344|   1.0|19.0|0.13570516|  39.07378|-1540.0|       0.0|\n",
      "|8.583|8.583|   0.0|8.583|8.583|   0.0|20.0| 0.1428062|  70.22051|-1540.0|  71.13095|\n",
      "|8.471|8.471|   0.0|8.471|8.471|   1.0|21.0|0.13945703|  43.57233|-1540.0| 58.435093|\n",
      "|8.564|8.564|   0.0|8.564|8.564|   1.0|22.0|0.14101775| 54.995224|-1540.0| 74.774734|\n",
      "|8.828|8.828|   0.0|8.828|8.828|   1.0|23.0| 0.1574003|  106.4896|-1540.0|  76.11935|\n",
      "|8.929|8.929|   0.0|8.929|8.929|   0.0|24.0|0.17465937| 113.93231|-1540.0|     100.0|\n",
      "|8.902|8.902|   0.0|8.902|8.902|   1.0|25.0|0.18407641|  96.88768|-1540.0| 93.112366|\n",
      "|9.092|9.092|   0.0|9.092|9.092|   0.0|26.0| 0.2016508| 124.93189|-1540.0|  91.50957|\n",
      "|9.057|9.057|   0.0|9.057|9.057|   0.0|27.0| 0.2104398|  97.59585|-1540.0|  75.39699|\n",
      "| 8.93| 8.93|   0.0| 8.93| 8.93|   1.0|28.0| 0.2061091| 59.030437|-1540.0| 53.977264|\n",
      "|9.012|9.012|   0.0|9.012|9.012|   1.0|29.0| 0.2057468|  72.02796|-1540.0| 33.606537|\n",
      "|8.986|8.986|   0.0|8.986|8.986|   0.0|31.0|0.20133945|  45.81469|-1540.0|  59.39592|\n",
      "|8.773|8.773|   0.0|8.773|8.773|   1.0|32.0|0.17788865|-14.703465|-1540.0|  22.14454|\n",
      "|8.816|8.816|   0.0|8.816|8.816|   1.0|33.0|0.16044436|-14.760238|-1540.0|11.4058695|\n",
      "|9.042|9.042|   0.0|9.042|9.042|   0.0|34.0|0.16065717| 52.015152|-1540.0| 55.809097|\n",
      "+-----+-----+------+-----+-----+------+----+----------+----------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if CCI_i:\n",
    "    converted_df = converted_df.filter(converted_df.CCI != \"NaN\")\n",
    "    \n",
    "if OBV_i:\n",
    "    converted_df = converted_df.filter(converted_df.OBV != \"NaN\")\n",
    "\n",
    "converted_df = converted_df.filter(converted_df.Profit != \"NaN\")\n",
    "\n",
    "converted_df.filter(converted_df.Profit == \"NaN\").show()\n",
    "\n",
    "converted_df = converted_df.sort(converted_df.id.asc())\n",
    "\n",
    "converted_df.show()\n",
    "\n",
    "# Manual split for training and validating data\n",
    "if ManualSplit:\n",
    "    dfp = converted_df.toPandas()\n",
    "\n",
    "    dfp = np.array_split(dfp, 10)\n",
    "\n",
    "    p0 = spark.createDataFrame(data=dfp[0])\n",
    "    p1 = spark.createDataFrame(data=dfp[1])\n",
    "    p2 = spark.createDataFrame(data=dfp[2])\n",
    "    p3 = spark.createDataFrame(data=dfp[3])\n",
    "    p4 = spark.createDataFrame(data=dfp[4])\n",
    "    p5 = spark.createDataFrame(data=dfp[5])\n",
    "    p6 = spark.createDataFrame(data=dfp[6])\n",
    "    p7 = spark.createDataFrame(data=dfp[7])\n",
    "    p8 = spark.createDataFrame(data=dfp[8])\n",
    "    p9 = spark.createDataFrame(data=dfp[9])\n",
    "\n",
    "    p_final = p0.union(p1).union(p2).union(p3).union(p4).union(p5).union(\n",
    "        p6).union(p7).union(p8)\n",
    "    train = p_final\n",
    "    test = p9\n",
    "    #     test = p9.head(10)\n",
    "    #     test = spark.createDataFrame(test)\n",
    "else:\n",
    "    train, test = converted_df.randomSplit([0.9, 0.1],seed=RANDOM_SEED)\n",
    "\n",
    "print(\"We have %d training examples and %d test examples.\" % (train.count(),\n",
    "                                                              test.count()))\n",
    "test = test.select(\n",
    "        [col(c).cast('float') for c in test.columns])\n",
    "\n",
    "train = train.select(\n",
    "        [col(c).cast('float') for c in train.columns])\n",
    "\n",
    "test = test.sort(test.id.asc())\n",
    "\n",
    "train = train.sort(train.id.asc())\n",
    "\n",
    "\n",
    "train.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Open', 'High', 'Volume', 'Low', 'Close', 'MACD', 'CCI', 'OBV', 'RSI']\n"
     ]
    }
   ],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "\n",
    "featuresCols = converted_df.columns\n",
    "featuresCols.remove('Profit')\n",
    "featuresCols.remove('id')\n",
    "\n",
    "print(featuresCols)\n",
    "\n",
    "# Vector Assembler\n",
    "# This concatenates all feature columns into a single feature vector in a new column \"rawFeatures\".\n",
    "# Used for assembling features into a vector.\n",
    "# We will pass all the columns that we are going to use for the prediction to the VectorAssembler and\n",
    "# it will create a new vector column.\n",
    "vectorAssembler_rt = VectorAssembler(\n",
    "    inputCols=featuresCols, outputCol=\"rawFeatures\")\n",
    "\n",
    "# VectorIndexer:\n",
    "# is used to index categorical predictors in a featuresCol column.\n",
    "# Remember that featuresCol is a single column consisting of vectors (refer to featuresCol and labelCol).\n",
    "# Each row is a vector which contains values from each predictors.\n",
    "\n",
    "featureIndexer_rt = VectorIndexer(\n",
    "    inputCol=\"rawFeatures\",\n",
    "    outputCol=\"features\",\n",
    "    maxCategories=len(featuresCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Algorithm\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "rt = DecisionTreeClassifier(labelCol='Profit', featuresCol=\"features\", minInfoGain=0.01)\n",
    "max_Depth_Range = list(range(8, 20))\n",
    "min_InstancesPerNode = list(range(1, 5))\n",
    "\n",
    "paramGrid_rt = ParamGridBuilder() \\\n",
    "    .addGrid(rt.maxDepth, max_Depth_Range) \\\n",
    "    .addGrid(rt.maxMemoryInMB, [1000] ).build()\n",
    "\n",
    "# We define an evaluation metric. This tells CrossValidator how well we are doing by comparing the true\n",
    "# labels with predictions.\n",
    "\n",
    "evaluator_rt = BinaryClassificationEvaluator(\n",
    "    labelCol=rt.getLabelCol(),\n",
    "    metricName='areaUnderROC',\n",
    "    rawPredictionCol=rt.getRawPredictionCol())\n",
    "\n",
    "evaluator_rt_PR = BinaryClassificationEvaluator(\n",
    "    labelCol=rt.getLabelCol(),\n",
    "    metricName='areaUnderPR',\n",
    "    rawPredictionCol=rt.getRawPredictionCol())\n",
    "\n",
    "# Declare the CrossValidator, which runs model tuning for us.\n",
    "cv_rt = CrossValidator(\n",
    "    estimator=rt,\n",
    "    evaluator=evaluator_rt,\n",
    "    estimatorParamMaps=paramGrid_rt,\n",
    "    numFolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "\n",
    "DecisionTreeClassificationModel.getMaxDepth = (\n",
    "    lambda self: self._java_obj.getMaxDepth())\n",
    "\n",
    "DecisionTreeClassificationModel.getMinInstancesPerNode = (\n",
    "    lambda self: self._java_obj.getMinInstancesPerNode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# if DEBUG:\n",
    "#     train.repartition(1).write.csv(\"PreProcessedSets/TrainSet_\" + str(time.mktime(datetime.datetime.today().timetuple())) + \"_.csv\", header = 'True')\n",
    "#     test.repartition(1).write.csv(\"PreProcessedSets/TestSet_\" + str(time.mktime(datetime.datetime.today().timetuple())) + \"_.csv\", header = 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Creating Final pipeline object\n",
    "pipeline_rt = Pipeline(stages=[vectorAssembler_rt, featureIndexer_rt, cv_rt])\n",
    "\n",
    "# FITTING!\n",
    "pipelineModel_rt = pipeline_rt.fit(train)\n",
    "\n",
    "# Getting the Best Model\n",
    "best_classifier = pipelineModel_rt.stages[-1].bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features importances(9,[1,2,5,6],[0.0648853714252,0.823072376954,0.0796102907628,0.0324319608578])\n",
      "Maximal depth is 8\n",
      "Minimal instances per node is 1\n",
      "AreaUnderROC on our test set: 0.557933\n",
      "AreaUnderPR on our test set: 0.7344\n",
      "Test count | FN | TN | TP | TN\n",
      "129 | 6 | 49 | 58 | 16\n",
      "+------+------+------+------+------+------+-----+-----------+----------+-------+---------+--------------------+--------------------+-------------+--------------------+----------+\n",
      "|  Open|  High|Volume|   Low| Close|Profit|   id|       MACD|       CCI|    OBV|      RSI|         rawFeatures|            features|rawPrediction|         probability|prediction|\n",
      "+------+------+------+------+------+------+-----+-----------+----------+-------+---------+--------------------+--------------------+-------------+--------------------+----------+\n",
      "| 8.404| 8.457|1250.0| 8.404| 8.441|   0.0| 17.0| 0.14652483|  67.58636|-1540.0|61.077244|[8.40400028228759...|[8.40400028228759...| [141.0,31.0]|[0.81976744186046...|       0.0|\n",
      "| 9.107| 9.107|   0.0| 9.107| 9.107|   0.0| 30.0| 0.20939665|  82.11697|-1540.0|58.223717|[9.10700035095214...|[9.10700035095214...|[440.0,586.0]|[0.42884990253411...|       1.0|\n",
      "| 9.337| 9.337|   0.0| 9.337| 9.337|   1.0| 51.0| 0.12940782|  104.0078|-1390.0|    100.0|[9.33699989318847...|[9.33699989318847...|[440.0,586.0]|[0.42884990253411...|       1.0|\n",
      "| 9.699| 9.699|   0.0| 9.699| 9.699|   0.0| 52.0| 0.16757031| 137.44931|-1390.0|    100.0|[9.69900035858154...|[9.69900035858154...|[440.0,586.0]|[0.42884990253411...|       1.0|\n",
      "| 9.166| 9.166|   0.0| 9.166| 9.166|   0.0| 60.0| 0.12811199|-59.121326|-1390.0|      0.0|[9.16600036621093...|[9.16600036621093...|[440.0,586.0]|[0.42884990253411...|       1.0|\n",
      "|10.403|10.403|   0.0|10.403|10.403|   1.0| 76.0| 0.27794483|  98.70771|-1390.0| 88.29808|[10.4029998779296...|[10.4029998779296...|[440.0,586.0]|[0.42884990253411...|       1.0|\n",
      "| 11.01| 11.01|   0.0| 11.01| 11.01|   0.0| 99.0| 0.21661694|0.26114646|-1390.0| 40.31828|[11.0100002288818...|[11.0100002288818...|[440.0,586.0]|[0.42884990253411...|       1.0|\n",
      "| 10.15| 10.15|   0.0| 10.15| 10.15|   1.0|106.0|  -0.076273|    -76.68|-4315.0|47.983818|[10.1499996185302...|[10.1499996185302...|[440.0,586.0]|[0.42884990253411...|       1.0|\n",
      "|10.126|10.126|   0.0|10.126|10.126|   1.0|117.0|-0.12650414|-19.228064|-4315.0| 59.77963|[10.1260004043579...|[10.1260004043579...|[440.0,586.0]|[0.42884990253411...|       1.0|\n",
      "|11.859|11.859|   0.0|11.859|11.859|   1.0|138.0|  0.1958685| 165.41418|-4455.0|    100.0|[11.8590002059936...|[11.8590002059936...|[440.0,586.0]|[0.42884990253411...|       1.0|\n",
      "|12.676|12.676|   0.0|12.676|12.676|   0.0|145.0| 0.41179407| 95.084274|-4430.0| 96.12276|[12.6759996414184...|[12.6759996414184...|[440.0,586.0]|[0.42884990253411...|       1.0|\n",
      "|11.809|11.809|   0.0|11.809|11.809|   1.0|170.0|0.016241657|-69.878136|-4120.0| 18.38577|[11.8090000152587...|[11.8090000152587...|[440.0,586.0]|[0.42884990253411...|       1.0|\n",
      "|12.106|12.106|   0.0|12.106|12.106|   1.0|182.0| 0.02176967|  72.51723|-3270.0|36.437275|[12.1059999465942...|[12.1059999465942...|[440.0,586.0]|[0.42884990253411...|       1.0|\n",
      "| 13.37| 13.37|   0.0| 13.37| 13.37|   1.0|193.0| 0.28479147| 108.33735|-2470.0|    100.0|[13.3699998855590...|[13.3699998855590...|[440.0,586.0]|[0.42884990253411...|       1.0|\n",
      "|13.315|13.315|   0.0|13.315|13.315|   1.0|195.0| 0.31755605|  71.53708|-2270.0|  63.9718|[13.3149995803833...|[13.3149995803833...|[440.0,586.0]|[0.42884990253411...|       1.0|\n",
      "|13.216|13.216|   0.0|13.216|13.216|   0.0|198.0|  0.3100778| 24.055878|-1960.0|45.272205|[13.2159996032714...|[13.2159996032714...|[440.0,586.0]|[0.42884990253411...|       1.0|\n",
      "|12.088|12.088|   0.0|12.088|12.088|   0.0|211.0|-0.13841897| -77.34863|-1620.0|      0.0|[12.0880002975463...|[12.0880002975463...|[440.0,586.0]|[0.42884990253411...|       1.0|\n",
      "| 11.38| 11.38|   0.0| 11.38| 11.38|   1.0|225.0|-0.23075853| -117.6517|-1620.0|      0.0|[11.3800001144409...|[11.3800001144409...|[440.0,586.0]|[0.42884990253411...|       1.0|\n",
      "|11.778| 12.01| 170.0|11.778| 12.01|   0.0|227.0|-0.19721523| 1.8767673|-1050.0|96.625725|[11.7779998779296...|[11.7779998779296...| [141.0,31.0]|[0.81976744186046...|       0.0|\n",
      "|11.661|11.845| 600.0|11.661|11.845|   0.0|229.0|-0.17851174|-21.785112| -450.0|61.144966|[11.6610002517700...|[11.6610002517700...| [141.0,31.0]|[0.81976744186046...|       0.0|\n",
      "+------+------+------+------+------+------+-----+-----------+----------+-------+---------+--------------------+--------------------+-------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Feature importance\n",
    "print('Features importances' + str(best_classifier.featureImportances))\n",
    "\n",
    "# Max depth\n",
    "print(\"Maximal depth is \" + str(best_classifier.getMaxDepth()))\n",
    "max_depth = best_classifier.getMaxDepth()\n",
    "\n",
    "# Min instances\n",
    "print(\"Minimal instances per node is \" + str(best_classifier.getMinInstancesPerNode()))\n",
    "min_instancesPerNode = best_classifier.getMinInstancesPerNode()\n",
    "\n",
    "# Making Predictions!\n",
    "predictions_rt = pipelineModel_rt.transform(test)\n",
    "\n",
    "# Calculating metrics\n",
    "AreaUnderROC = evaluator_rt.evaluate(predictions_rt)\n",
    "print(\"AreaUnderROC on our test set: %g\" % AreaUnderROC)\n",
    "\n",
    "# Calculating metrics\n",
    "AreaUnderPR = evaluator_rt_PR.evaluate(predictions_rt)\n",
    "print(\"AreaUnderPR on our test set: %g\" % AreaUnderPR)\n",
    "\n",
    "#evaluate results\n",
    "testCount = predictions_rt.count()\n",
    "\n",
    "FP = predictions_rt.where(\"prediction = 0 AND Profit=1\").count() #FN\n",
    "FN = predictions_rt.where(\"prediction = 1 AND Profit=0\").count() #TN\n",
    "TP = predictions_rt.where(\"prediction = 1 AND Profit=1\").count() #TP\n",
    "TN = predictions_rt.where(\"prediction = 0 AND Profit=0\").count() #TN\n",
    "\n",
    "print(\"Test count | FN | TN | TP | TN\")\n",
    "print(str(testCount)+\" | \"+str(FP)+\" | \"+str(FN)+\" | \"+str(TP)+\" | \"+str(TN)) \n",
    "\n",
    "predictions_rt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    train, test = converted_df.randomSplit([0.9, 0.1],seed=i+1)\n",
    "    featuresCols = converted_df.columns\n",
    "    featuresCols.remove('Profit')\n",
    "    featuresCols.remove('id')\n",
    "    vectorAssembler_rt = VectorAssembler(\n",
    "        inputCols=featuresCols, outputCol=\"rawFeatures\")\n",
    "    \n",
    "    featureIndexer_rt = VectorIndexer(\n",
    "        inputCol=\"rawFeatures\",\n",
    "        outputCol=\"features\",\n",
    "        maxCategories=len(featuresCols))\n",
    "    dt = DecisionTreeClassifier(labelCol='Profit', featuresCol=\"features\", maxDepth = 8, minInstancesPerNode = 1, minInfoGain=0.01)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o73855.csv.\n: java.lang.UnsupportedOperationException: CSV data source does not support struct<type:tinyint,size:int,indices:array<int>,values:array<double>> data type.\n\tat org.apache.spark.sql.execution.datasources.csv.CSVUtils$.org$apache$spark$sql$execution$datasources$csv$CSVUtils$$verifyType$1(CSVUtils.scala:127)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVUtils$$anonfun$verifySchema$1.apply(CSVUtils.scala:131)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVUtils$$anonfun$verifySchema$1.apply(CSVUtils.scala:131)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:98)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVUtils$.verifySchema(CSVUtils.scala:131)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.prepareWrite(CSVFileFormat.scala:65)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:142)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:598)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-411-1a6e47538a79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     \"s3://logs102/Models/Model_DT_MI_{}_{}\".format(str(best_classifier.getMaxDepth()), str(best_classifier.getMinInstancesPerNode())))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpredictions_rt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/ec2-user/Models/DT_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmktime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimetuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'True'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace)\u001b[0m\n\u001b[1;32m    764\u001b[0m                        \u001b[0mignoreLeadingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignoreLeadingWhiteSpace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m                        ignoreTrailingWhiteSpace=ignoreTrailingWhiteSpace)\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o73855.csv.\n: java.lang.UnsupportedOperationException: CSV data source does not support struct<type:tinyint,size:int,indices:array<int>,values:array<double>> data type.\n\tat org.apache.spark.sql.execution.datasources.csv.CSVUtils$.org$apache$spark$sql$execution$datasources$csv$CSVUtils$$verifyType$1(CSVUtils.scala:127)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVUtils$$anonfun$verifySchema$1.apply(CSVUtils.scala:131)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVUtils$$anonfun$verifySchema$1.apply(CSVUtils.scala:131)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:98)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVUtils$.verifySchema(CSVUtils.scala:131)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.prepareWrite(CSVFileFormat.scala:65)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:142)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:598)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# # Compute raw scores on the test set\n",
    "# predictionAndLabels = test.map(lambda lp: (float(best_classifier.predict(lp.features)), lp.label))\n",
    "# if DT:\n",
    "#     # Saving best model to file\n",
    "#     best_classifier.write(\n",
    "#     \"s3://logs102/Models/Model_DT_MI_{}_{}\".format(str(best_classifier.getMaxDepth()), str(best_classifier.getMinInstancesPerNode())))\n",
    "\n",
    "predictions_rt.repartition(1).write.csv(\"/home/ec2-user/Models/DT_\" + str(time.mktime(datetime.datetime.today().timetuple())) + \"_.csv\",mode = 'overwrite', header= 'True')\n",
    "\n",
    "if DEBUG:\n",
    "    df_to_plot_rt = predictions_rt.select('prediction', 'Profit')\n",
    "#     print(df_to_plot_rt)\n",
    "    df_to_plot_rt = df_to_plot_rt.toPandas()\n",
    "    plt_dt.figure(figsize=(14, 14))\n",
    "    plt_dt.plot(df_to_plot_rt)\n",
    "    plt_dt.legend(df_to_plot_rt.columns)\n",
    "    plt_dt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
