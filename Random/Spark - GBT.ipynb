{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SQLContext\n",
    "spark = SparkSession.builder.config(conf=SparkConf()).getOrCreate()\n",
    "# Print config\n",
    "# print(spark.sparkContext.getConf().getAll())\n",
    "path_to_csv = \"./Datasets/Orlen.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(path_to_csv, header=True, inferSchema=True)\n",
    "# print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleared = df.drop(\"Adj Close\")\n",
    "# print(df_cleared.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "df_lag = df_cleared.withColumn('prev_day_price',\n",
    "                               func.lag(df_cleared['Close']).over(\n",
    "                                   Window.orderBy(\"Date\")))\n",
    "# df_lag.show()\n",
    "\n",
    "df_with_prev_day_price = df_lag.withColumn(\n",
    "    'daily_return',\n",
    "    (df_lag['Close'] - df_lag['prev_day_price']) / df_lag['Close'])\n",
    "# df_with_prev_day_price.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "labeled = df_with_prev_day_price.withColumn(\n",
    "    'profit',\n",
    "    (F.when(df_with_prev_day_price[\"daily_return\"] < 0, 0).otherwise(1)))\n",
    "# labeled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df = labeled.where(labeled[\"daily_return\"].isNotNull())\n",
    "result_df = final_df\n",
    "# final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "final_df = final_df.drop(\"prev_day_price\").withColumnRenamed(\n",
    "    existing=\"daily_return\", new=\"Daily return\")\n",
    "\n",
    "converted_df = final_df.select(\n",
    "    final_df[\"Open\"].cast(\"float\"), final_df[\"High\"].cast(\"float\"),\n",
    "    final_df[\"Low\"].cast(\"float\"), final_df[\"Close\"].cast(\"float\"),\n",
    "    final_df[\"Daily return\"].cast(\"float\"), final_df[\"profit\"].cast(\"int\"))\n",
    "\n",
    "train, test = converted_df.randomSplit([0.1, 0.9])\n",
    "print(\"We have %d training examples and %d test examples.\" % (train.count(),\n",
    "                                                              test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "featuresCols = converted_df.columns\n",
    "featuresCols.remove('profit')\n",
    "print(featuresCols)\n",
    "\n",
    "# This concatenates all feature columns into a single feature vector in a new column \"rawFeatures\".\n",
    "vectorAssembler_dt = VectorAssembler(\n",
    "    inputCols=featuresCols, outputCol=\"rawFeatures\")\n",
    "# This identifies categorical features and indexes them.\n",
    "vectorIndexer_dt = VectorIndexer(\n",
    "    inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=4)\n",
    "\n",
    "dt = DecisionTreeRegressor(labelCol=\"Close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Define a grid of hyperparameters to test:\n",
    "#  - maxDepth: max depth of each decision tree in the GBT ensemble\n",
    "# In this example notebook, we keep these values small.\n",
    "# In practice, to get the highest accuracy, you would likely want to try deeper trees (10 or higher)\n",
    "# and more trees in the ensemble (>100).\n",
    "\n",
    "max_Depth_Range = list(range(1, 10))\n",
    "min_InstancesPerNode = list(range(1, 8))\n",
    "max_Bins = list(range(16, 48))\n",
    "\n",
    "\n",
    "paramGrid_dt = ParamGridBuilder()\\\n",
    "    .addGrid(dt.maxDepth, max_Depth_Range)\\\n",
    "    .addGrid(dt.maxBins,max_Bins )\\\n",
    "    .addGrid(dt.minInstancesPerNode,min_InstancesPerNode )\\\n",
    "    .addGrid(dt.maxMemoryInMB, [1500]).build()\n",
    "\n",
    "# We define an evaluation metric.  This tells CrossValidator how well we are doing by comparing the true\n",
    "# labels with predictions.\n",
    "evaluator_dt = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=dt.getLabelCol(),\n",
    "    predictionCol=dt.getPredictionCol())\n",
    "\n",
    "# Declare the CrossValidator, which runs model tuning for us.\n",
    "cv_dt = CrossValidator(\n",
    "    estimator=dt, evaluator=evaluator_dt, estimatorParamMaps=paramGrid_dt)\n",
    "\n",
    "pipeline_dt = Pipeline(stages=[vectorAssembler_dt, vectorIndexer_dt, cv_dt])\n",
    "\n",
    "pipelineModel_dt = pipeline_dt.fit(train)\n",
    "\n",
    "predictions_dt = pipelineModel_dt.transform(test)\n",
    "\n",
    "rmse_dt = evaluator_dt.evaluate(predictions_dt)\n",
    "\n",
    "print(\"RMSE on our test set: %g\" % rmse_dt)\n",
    "\n",
    "df_to_plot_dt = predictions_dt.drop(\"Open\", \"High\", \"Low\", \"Daily return\",\n",
    "                                    \"profit\", \"rawFeatures\", 'features')\n",
    "df_to_plot_dt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt_dt\n",
    "df_to_plot_dt = df_to_plot_dt.toPandas()\n",
    "plt_dt.figure(figsize=(14, 14))\n",
    "plt_dt.plot(df_to_plot_dt)\n",
    "plt_dt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.show()\n",
    "# print(predictions.schema)\n",
    "# predictions.select('features').show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest model.\n",
    "# Random Forest\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Define a grid of hyperparameters to test:\n",
    "#  - maxDepth: max depth of each decision tree in the GBT ensemble\n",
    "# In this example notebook, we keep these values small.\n",
    "# In practice, to get the highest accuracy, you would likely want to try deeper trees (10 or higher)\n",
    "# and more trees in the ensemble (>100).\n",
    "\n",
    "featuresCols = converted_df.columns\n",
    "featuresCols.remove('profit')\n",
    "# print(featuresCols)\n",
    "\n",
    "# This concatenates all feature columns into a single feature vector in a new column \"rawFeatures\".\n",
    "vectorAssembler_rf = VectorAssembler(\n",
    "    inputCols=featuresCols, outputCol=\"rawFeatures\")\n",
    "# This identifies categorical features and indexes them.\n",
    "vectorIndexer_rf = VectorIndexer(\n",
    "    inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=4)\n",
    "\n",
    "rf = RandomForestRegressor(labelCol='Close', featuresCol=\"features\")\n",
    "\n",
    "paramGrid_rf = ParamGridBuilder()\\\n",
    "    .addGrid(rf.maxDepth, [2,10,20])\\\n",
    "    .addGrid(rf.maxBins, [16, 32, 64 ,128])\\\n",
    "    .addGrid(rf.minInfoGain, [0.1])\\\n",
    "    .build()\n",
    "\n",
    "# We define an evaluation metric.  This tells CrossValidator how well we are doing by comparing the true\n",
    "# labels with predictions.\n",
    "evaluator_rf = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=rf.getLabelCol(),\n",
    "    predictionCol=rf.getPredictionCol())\n",
    "\n",
    "# Declare the CrossValidator, which runs model tuning for us.\n",
    "cv_rf = CrossValidator(\n",
    "    estimator=rf, evaluator=evaluator_rf, estimatorParamMaps=paramGrid_rf)\n",
    "\n",
    "pipeline_rf = Pipeline(stages=[vectorAssembler_rf, vectorIndexer_rf, cv_rf])\n",
    "\n",
    "pipelineModel_rf = pipeline_rf.fit(train)\n",
    "\n",
    "predictions_rf = pipelineModel_rf.transform(test)\n",
    "\n",
    "rmse_rf = evaluator_rf.evaluate(predictions_rf)\n",
    "\n",
    "print(\"RMSE on our test set: %g\" % rmse_rf)\n",
    "\n",
    "df_to_plot_rf = predictions_rf.drop(\"Open\", \"High\", \"Low\", \"Daily return\",\n",
    "                                    \"profit\", \"rawFeatures\", 'features')\n",
    "\n",
    "import matplotlib.pyplot as plt_rf\n",
    "df_to_plot_rf = df_to_plot_rf.toPandas()\n",
    "plt_rf.figure(figsize=(14, 14))\n",
    "plt_rf.plot(df_to_plot_rf)\n",
    "plt_rf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a GBTrees model.\n",
    "# Random Forest\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Define a grid of hyperparameters to test:\n",
    "#  - maxDepth: max depth of each decision tree in the GBT ensemble\n",
    "# In this example notebook, we keep these values small.\n",
    "# In practice, to get the highest accuracy, you would likely want to try deeper trees (10 or higher)\n",
    "# and more trees in the ensemble (>100).\n",
    "\n",
    "featuresCols = converted_df.columns\n",
    "featuresCols.remove('profit')\n",
    "# print(featuresCols)\n",
    "\n",
    "# This concatenates all feature columns into a single feature vector in a new column \"rawFeatures\".\n",
    "vectorAssembler_gbt = VectorAssembler(\n",
    "    inputCols=featuresCols, outputCol=\"rawFeatures\")\n",
    "# This identifies categorical features and indexes them.\n",
    "vectorIndexer_gbt = VectorIndexer(\n",
    "    inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=4)\n",
    "\n",
    "gbt = GBTRegressor(labelCol='Close', featuresCol=\"features\")\n",
    "\n",
    "paramGrid_gbt = ParamGridBuilder()\\\n",
    "    .addGrid(gbt.maxDepth, [2, 5])\\\n",
    "    .addGrid(gbt.maxIter, [10, 20])\\\n",
    "    .build()\n",
    "\n",
    "# We define an evaluation metric.  This tells CrossValidator how well we are doing by comparing the true\n",
    "# labels with predictions.\n",
    "evaluator_gbt = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=gbt.getLabelCol(),\n",
    "    predictionCol=gbt.getPredictionCol())\n",
    "\n",
    "# Declare the CrossValidator, which runs model tuning for us.\n",
    "cv_gbt = CrossValidator(\n",
    "    estimator=gbt, evaluator=evaluator_gbt, estimatorParamMaps=paramGrid_gbt)\n",
    "\n",
    "pipeline_gbt = Pipeline(\n",
    "    stages=[vectorAssembler_gbt, vectorIndexer_gbt, cv_gbt])\n",
    "\n",
    "pipelineModel_gbt = pipeline_gbt.fit(train)\n",
    "\n",
    "predictions_gbt = pipelineModel_gbt.transform(test)\n",
    "\n",
    "rmse_gbt = evaluator_gbt.evaluate(predictions_gbt)\n",
    "\n",
    "print(\"RMSE on our test set: %g\" % rmse_gbt)\n",
    "\n",
    "df_to_plot_gbt = predictions_gbt.drop(\"Open\", \"High\", \"Low\", \"Daily return\",\n",
    "                                      \"profit\", \"rawFeatures\", 'features')\n",
    "\n",
    "import matplotlib.pyplot as plt_gbt\n",
    "df_to_plot_gbt = df_to_plot_gbt.toPandas()\n",
    "plt_gbt.figure(figsize=(14, 14))\n",
    "plt_gbt.plot(df_to_plot_gbt)\n",
    "plt_gbt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----+-----+-----+-----+-------------+------+---+\n",
      "|      RSI| id| Open| High|  Low|Close| Daily return|profit| id|\n",
      "+---------+---+-----+-----+-----+-----+-------------+------+---+\n",
      "|      0.0|  0|7.802|7.809|7.802|7.809|  0.013189909|     1|  0|\n",
      "|      0.0|  1| 7.95| 7.95|7.782|7.782|-0.0034695452|     0|  1|\n",
      "|      0.0|  2|7.688|7.688|7.688|7.688| -0.012226847|     0|  2|\n",
      "|      0.0|  3|7.526|7.526|7.459|7.459| -0.030701166|     0|  3|\n",
      "|      0.0|  4|7.328|7.328|7.328|7.328| -0.017876638|     0|  4|\n",
      "|      0.0|  5|7.514|7.514|7.514|7.514|  0.024753792|     1|  5|\n",
      "|      0.0|  6|7.317|7.317|7.317|7.317| -0.026923602|     0|  6|\n",
      "|      0.0|  7|7.502|7.569|7.502|7.569|  0.033293698|     1|  7|\n",
      "|      0.0|  8|7.701|7.701|7.701|7.701|   0.01714063|     1|  8|\n",
      "|      0.0|  9|7.671|7.671|7.671|7.671| -0.003910833|     0|  9|\n",
      "|      0.0| 10|7.935|7.935|7.935|7.935|   0.03327032|     1| 10|\n",
      "|      0.0| 11|7.978|7.978|7.978|7.978|  0.005389822|     1| 11|\n",
      "|      0.0| 12|8.089|8.089|8.089|8.089|  0.013722339|     1| 12|\n",
      "|      0.0| 13|8.337|8.337|8.337|8.337|   0.02974691|     1| 13|\n",
      "|60.058296| 14|8.223|8.223|8.223|8.223| -0.013863553|     0| 14|\n",
      "| 69.79483| 15|8.587|8.824|8.587|8.824|    0.0681097|     1| 15|\n",
      "| 66.89729| 16|8.634|8.634|8.616|8.616| -0.024141133|     0| 16|\n",
      "| 68.23921| 17|8.404|8.457|8.404|8.441|  -0.02073214|     0| 17|\n",
      "| 70.65384| 18|8.402|8.402|8.402|8.402|-0.0046417518|     0| 18|\n",
      "| 66.78801| 19|8.344|8.344|8.344|8.344|-0.0069511025|     0| 19|\n",
      "+---------+---+-----+-----+-----+-----+-------------+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zapo/Documents/Spark/spark/lib/python3.6/site-packages/ipykernel_launcher.py:28: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=14,center=False).mean()\n",
      "/home/zapo/Documents/Spark/spark/lib/python3.6/site-packages/ipykernel_launcher.py:29: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=14,center=False).mean()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas_datareader import data, wb\n",
    "import datetime\n",
    "from pyspark import SQLContext\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def RSI(dataframe, column, window_length, avg_type):\n",
    "    data = dataframe.toPandas()\n",
    "    # Get just the close\n",
    "    close = data[column]\n",
    "    # Get the difference in price from previous step\n",
    "    delta = close.diff()\n",
    "    # Get rid of the first row, which is NaN since it did not have a previous\n",
    "    # row to calculate the differences\n",
    "    # Make the positive gains (up) and negative gains (down) Series\n",
    "    up, down = delta.copy(), delta.copy()\n",
    "    up[up < 0] = 0\n",
    "    down[down > 0] = 0\n",
    "    if avg_type == \"EWMA\":\n",
    "        roll_up = up.ewm(com=14).mean()\n",
    "        roll_down = down.abs().ewm(com=14).mean()\n",
    "        RS = roll_up / roll_down\n",
    "        RSI = 100.0 - (100.0 / (1.0 + RS))\n",
    "    elif avg_type == \"SMA\":\n",
    "        roll_up = pd.rolling_mean(up, window_length)\n",
    "        roll_down = pd.rolling_mean(down.abs(), window_length)\n",
    "        RS = roll_up / roll_down\n",
    "        RSI = 100.0 - (100.0 / (1.0 + RS))\n",
    "    from pyspark.sql.types import FloatType\n",
    "    temp_df = spark.createDataFrame(data=RSI, schema=FloatType()).fillna(0)\n",
    "    from pyspark.sql.functions import monotonically_increasing_id\n",
    "    df1 = temp_df.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\n",
    "    df1 = df1.withColumnRenamed('value', 'RSI')\n",
    "    df2 = converted_df.select(\"*\").withColumn(\"id\",\n",
    "                                              monotonically_increasing_id())\n",
    "    joined_df = df1.join(df2, df1.id == df2.id)\n",
    "    return joined_df\n",
    "\n",
    "\n",
    "test = RSI(converted_df, \"Close\", 14, 'SMA')\n",
    "\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
