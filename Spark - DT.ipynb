{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-07353c6b504a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Spark libs\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark import SQLContext\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Others\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Graphs libs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt_dt\n",
    "\n",
    "# S3 Service\n",
    "# import boto3\n",
    "# from io import StringIO\n",
    "\n",
    "# # Let's use Amazon S3\n",
    "# s3 = boto3.resource('s3')\n",
    "\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "OBV_i = True\n",
    "CCI_i = True\n",
    "RSI_i = True\n",
    "MACD_i = True\n",
    "ManualSplit = False\n",
    "Date_Convert = False\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "DT = True\n",
    "RT = False\n",
    "GBT = False\n",
    "\n",
    "RANDOM_SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def RSI(dataframe, window_length, avg_type, column='Close'):\n",
    "    data = dataframe.toPandas()\n",
    "    # Get just the close\n",
    "    close = data['Close']\n",
    "    # Get the difference in price from previous step\n",
    "    delta = close.diff()\n",
    "    # Get rid of the first row, which is NaN since it did not have a previous\n",
    "    # row to calculate the differences\n",
    "    # Make the positive gains (up) and negative gains (down) Series\n",
    "    up, down = delta.copy(), delta.copy()\n",
    "    up[up < 0] = 0\n",
    "    down[down > 0] = 0\n",
    "    if avg_type == \"EWMA\":\n",
    "        roll_up = up.ewm(span=window_length, min_periods=window_length).mean()\n",
    "        roll_down = down.abs().ewm(\n",
    "            span=window_length, min_periods=window_length).mean()\n",
    "    elif avg_type == \"SMA\":\n",
    "        roll_up = pd.rolling_mean(up, window_length)\n",
    "        roll_down = pd.rolling_mean(down.abs(), window_length)\n",
    "    RS = roll_up / roll_down\n",
    "    RSI = 100.0 - (100.0 / (1.0 + RS))\n",
    "    RSI = pd.DataFrame({'RSI': RSI})\n",
    "    data = data.join(RSI)\n",
    "    result_df = spark.createDataFrame(data.round(3))\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Commodity Channel Index\n",
    "def CCI(spark_df, ndays):\n",
    "    data = spark_df.toPandas()\n",
    "    TP = (data['High'] + data['Low'] + data['Close']) / 3\n",
    "    CCI = pd.Series(\n",
    "        (TP - pd.rolling_mean(TP, ndays)) /\n",
    "        (0.015 * pd.rolling_std(TP, ndays)),\n",
    "        name='CCI')\n",
    "    data = data.join(CCI)\n",
    "    result_df = spark.createDataFrame(data.round(3))\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Moving average convergence divergence\n",
    "def MACD(dataframe, nfast=12, nslow=26, signal=9, column='Close'):\n",
    "    data = dataframe.toPandas()\n",
    "    # Get just the close\n",
    "    price = data[column]\n",
    "    # Get the difference in price from previous step\n",
    "    emaslow = pd.ewma(price, span=nslow, min_periods=1)\n",
    "    emafast = pd.ewma(price, span=nfast, min_periods=1)\n",
    "    #     MACD = pd.DataFrame({'MACD': emafast-emaslow, 'emaSlw': emaslow, 'emaFst': emafast})\n",
    "    MACD = pd.DataFrame({'MACD': emafast - emaslow})\n",
    "\n",
    "    data = data.join(MACD.round(3))\n",
    "    result_df = spark.createDataFrame(data)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF FUNCTIONS\n",
    "def profit_(today_price, previous_day_price):\n",
    "    if today_price - previous_day_price > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "profit_udf = udf(profit_, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+------+-----+-----+------+\n",
      "| id|      Date| Open| High|Volume|  Low|Close|Profit|\n",
      "+---+----------+-----+-----+------+-----+-----+------+\n",
      "|  0|2012-05-29|7.802|7.809|     0|7.802|7.809|     0|\n",
      "|  1|2012-05-30| 7.95| 7.95|    10|7.782|7.782|     0|\n",
      "|  2|2012-05-31|7.688|7.688|     0|7.688|7.688|     0|\n",
      "|  3|2012-06-01|7.526|7.526|   300|7.459|7.459|     0|\n",
      "|  4|2012-06-04|7.328|7.328|     0|7.328|7.328|     1|\n",
      "|  5|2012-06-05|7.514|7.514|     0|7.514|7.514|     0|\n",
      "|  6|2012-06-06|7.317|7.317|     0|7.317|7.317|     1|\n",
      "|  7|2012-06-08|7.502|7.569|     0|7.502|7.569|     1|\n",
      "|  8|2012-06-11|7.701|7.701|     0|7.701|7.701|     0|\n",
      "|  9|2012-06-12|7.671|7.671|     0|7.671|7.671|     1|\n",
      "| 10|2012-06-13|7.935|7.935|     0|7.935|7.935|     1|\n",
      "| 11|2012-06-14|7.978|7.978|     0|7.978|7.978|     1|\n",
      "| 12|2012-06-15|8.089|8.089|     0|8.089|8.089|     1|\n",
      "| 13|2012-06-18|8.337|8.337|     0|8.337|8.337|     0|\n",
      "| 14|2012-06-19|8.223|8.223|     0|8.223|8.223|     1|\n",
      "| 15|2012-06-20|8.587|8.824|   370|8.587|8.824|     0|\n",
      "| 16|2012-06-21|8.634|8.634|   350|8.616|8.616|     0|\n",
      "| 17|2012-06-22|8.404|8.457|  1250|8.404|8.441|     0|\n",
      "| 18|2012-06-25|8.402|8.402|     0|8.402|8.402|     0|\n",
      "| 19|2012-06-26|8.344|8.344|     0|8.344|8.344|     1|\n",
      "+---+----------+-----+-----+------+-----+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark context simple configuration\n",
    "spark = SparkSession.builder.config(conf=SparkConf()).getOrCreate()\n",
    "\n",
    "#Path to file with CSV\n",
    "\n",
    "# path_to_csv = \"s3://stocksets100/Orlen.csv\"\n",
    "path_to_csv = \"./Datasets/Orlen.csv\"\n",
    "\n",
    "fresh_df = spark.read.csv(path_to_csv, header=True, inferSchema=True)\n",
    "\n",
    "fresh_df = fresh_df.filter(fresh_df.Open != \"null\")\n",
    "\n",
    "temporary_df = fresh_df.select(\n",
    "    fresh_df[\"Date\"].cast(\"Date\"), fresh_df[\"Open\"].cast(\"float\"),\n",
    "    fresh_df[\"High\"].cast(\"float\"), fresh_df[\"Volume\"].cast(\"int\"),\n",
    "    fresh_df[\"Low\"].cast(\"float\"), fresh_df[\"Close\"].cast(\"float\"))\n",
    "\n",
    "# Creating new column with shifted Close price by 1 day\n",
    "df_daily_return = temporary_df.withColumn('prev_day_price',\n",
    "                                 func.lag(temporary_df['Close']).over(\n",
    "                                     Window.orderBy(\"Date\")))\n",
    "\n",
    "df_daily_return = df_daily_return.filter(\n",
    "    df_daily_return.prev_day_price.isNotNull())\n",
    "\n",
    "# Profit label calculation\n",
    "# 1 if stock risen up, 0 is it went down\n",
    "df_profit = df_daily_return.withColumn(\n",
    "    'Profit', profit_udf(df_daily_return.Close,\n",
    "                         df_daily_return.prev_day_price))\n",
    "\n",
    "df_shifted_profit = df_profit.withColumn(\n",
    "    'Profit',\n",
    "    func.lag(df_profit['Profit'], count=-1).over(Window.orderBy(\"Date\")))\n",
    "\n",
    "final_df = df_shifted_profit.filter(df_shifted_profit.Profit.isNotNull())\n",
    "\n",
    "# Removing redudant columns\n",
    "final_df = final_df.drop(\"Daily return\")\n",
    "final_df = final_df.drop(\"prev_day_price\")\n",
    "\n",
    "#Conversion to desired typesf\n",
    "converted_df = final_df.select(\"*\").withColumn(\"id\",\n",
    "                                               monotonically_increasing_id())\n",
    "\n",
    "converted_df = converted_df.select(\"id\", 'Date', 'Open', 'High', 'Volume',\n",
    "                                   'Low', 'Close', 'Profit')\n",
    "if DEBUG:\n",
    "    converted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "#Columns without Date\n",
    "# converted_df = converted_df.select(\n",
    "#     [col(c).cast('float') for c in converted_df.columns if c. not in {'Date'}])\n",
    "# Date column!\n",
    "df_date = converted_df.select(converted_df.Date)\n",
    "df_date = df_date.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "#Convert date to splitted format\n",
    "if Date_Convert:\n",
    "    split_col = pyspark.sql.functions.split(converted_df['Date'], '-')\n",
    "    converted_df = converted_df.withColumn('Year',\n",
    "                                           split_col.getItem(0).cast('int'))\n",
    "    converted_df = converted_df.withColumn('Month',\n",
    "                                           split_col.getItem(1).cast('int'))\n",
    "    converted_df = converted_df.withColumn('Day',\n",
    "                                           split_col.getItem(2).cast('int'))\n",
    "    if DEBUG:\n",
    "        converted_df.show()\n",
    "\n",
    "converted_df = converted_df.drop(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zapo/Documents/Spark/spark/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: pd.ewm_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.ewm(span=26,min_periods=1,adjust=True,ignore_na=False).mean()\n",
      "  import sys\n",
      "/home/zapo/Documents/Spark/spark/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: pd.ewm_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.ewm(span=12,min_periods=1,adjust=True,ignore_na=False).mean()\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+------+-----+-----+------+------+\n",
      "|  id| Open| High|Volume|  Low|Close|Profit|  MACD|\n",
      "+----+-----+-----+------+-----+-----+------+------+\n",
      "| 0.0|7.802|7.809|   0.0|7.802|7.809|   0.0|   0.0|\n",
      "| 1.0| 7.95| 7.95|  10.0|7.782|7.782|   0.0|-0.001|\n",
      "| 2.0|7.688|7.688|   0.0|7.688|7.688|   0.0|-0.004|\n",
      "| 3.0|7.526|7.526| 300.0|7.459|7.459|   0.0|-0.013|\n",
      "| 4.0|7.328|7.328|   0.0|7.328|7.328|   1.0|-0.023|\n",
      "| 5.0|7.514|7.514|   0.0|7.514|7.514|   0.0|-0.021|\n",
      "| 6.0|7.317|7.317|   0.0|7.317|7.317|   1.0|-0.028|\n",
      "| 7.0|7.502|7.569|   0.0|7.502|7.569|   1.0|-0.021|\n",
      "| 8.0|7.701|7.701|   0.0|7.701|7.701|   0.0|-0.008|\n",
      "| 9.0|7.671|7.671|   0.0|7.671|7.671|   1.0|-0.001|\n",
      "|10.0|7.935|7.935|   0.0|7.935|7.935|   1.0| 0.018|\n",
      "|11.0|7.978|7.978|   0.0|7.978|7.978|   1.0| 0.034|\n",
      "|12.0|8.089|8.089|   0.0|8.089|8.089|   1.0| 0.052|\n",
      "|13.0|8.337|8.337|   0.0|8.337|8.337|   0.0| 0.079|\n",
      "|14.0|8.223|8.223|   0.0|8.223|8.223|   1.0| 0.092|\n",
      "|15.0|8.587|8.824| 370.0|8.587|8.824|   0.0| 0.137|\n",
      "|16.0|8.634|8.634| 350.0|8.616|8.616|   0.0| 0.157|\n",
      "|17.0|8.404|8.457|1250.0|8.404|8.441|   0.0| 0.159|\n",
      "|18.0|8.402|8.402|   0.0|8.402|8.402|   0.0| 0.156|\n",
      "|19.0|8.344|8.344|   0.0|8.344|8.344|   1.0| 0.148|\n",
      "+----+-----+-----+------+-----+-----+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if MACD_i:\n",
    "    converted_df = MACD(converted_df)\n",
    "    converted_df = converted_df.select(\n",
    "        [col(c).cast('float') for c in converted_df.columns])\n",
    "    if DEBUG:\n",
    "        converted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zapo/Documents/Spark/spark/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=14,center=False).mean()\n",
      "  \n",
      "/home/zapo/Documents/Spark/spark/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: pd.rolling_std is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=14,center=False).std()\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+------+-----+-----+------+-----+-------+\n",
      "|  id| Open| High|Volume|  Low|Close|Profit| MACD|    CCI|\n",
      "+----+-----+-----+------+-----+-----+------+-----+-------+\n",
      "|13.0|8.337|8.337|   0.0|8.337|8.337|   0.0|0.079|139.019|\n",
      "|14.0|8.223|8.223|   0.0|8.223|8.223|   1.0|0.092| 96.656|\n",
      "|15.0|8.587|8.824| 370.0|8.587|8.824|   0.0|0.137|148.128|\n",
      "|16.0|8.634|8.634| 350.0|8.616|8.616|   0.0|0.157|105.222|\n",
      "|17.0|8.404|8.457|1250.0|8.404|8.441|   0.0|0.159| 67.586|\n",
      "|18.0|8.402|8.402|   0.0|8.402|8.402|   0.0|0.156| 54.927|\n",
      "|19.0|8.344|8.344|   0.0|8.344|8.344|   1.0|0.148| 39.074|\n",
      "|20.0|8.583|8.583|   0.0|8.583|8.583|   0.0|0.156| 70.221|\n",
      "|21.0|8.471|8.471|   0.0|8.471|8.471|   1.0|0.153| 43.572|\n",
      "|22.0|8.564|8.564|   0.0|8.564|8.564|   1.0|0.155| 54.995|\n",
      "|23.0|8.828|8.828|   0.0|8.828|8.828|   1.0|0.173| 106.49|\n",
      "|24.0|8.929|8.929|   0.0|8.929|8.929|   0.0|0.192|113.932|\n",
      "|25.0|8.902|8.902|   0.0|8.902|8.902|   1.0|0.202| 96.888|\n",
      "|26.0|9.092|9.092|   0.0|9.092|9.092|   0.0|0.222|124.932|\n",
      "|27.0|9.057|9.057|   0.0|9.057|9.057|   0.0|0.232| 97.596|\n",
      "|28.0| 8.93| 8.93|   0.0| 8.93| 8.93|   1.0|0.228|  59.03|\n",
      "|29.0|9.012|9.012|   0.0|9.012|9.012|   1.0|0.228| 72.028|\n",
      "|30.0|9.107|9.107|   0.0|9.107|9.107|   0.0|0.232| 82.117|\n",
      "|31.0|8.986|8.986|   0.0|8.986|8.986|   0.0|0.224| 45.815|\n",
      "|32.0|8.773|8.773|   0.0|8.773|8.773|   1.0|0.199|-14.703|\n",
      "+----+-----+-----+------+-----+-----+------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if CCI_i:\n",
    "    converted_df = CCI(converted_df, 14)\n",
    "    converted_df = converted_df.select(\n",
    "        [col(c).cast('float') for c in converted_df.columns])\n",
    "    converted_df = converted_df.filter(converted_df.CCI != \"NaN\")\n",
    "    if DEBUG:\n",
    "        converted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+------+-----+-----+------+-----+-------+-------+\n",
      "|  id| Open| High|Volume|  Low|Close|Profit| MACD|    CCI|    OBV|\n",
      "+----+-----+-----+------+-----+-----+------+-----+-------+-------+\n",
      "|13.0|8.337|8.337|   0.0|8.337|8.337|   0.0|0.079|139.019|    0.0|\n",
      "|14.0|8.223|8.223|   0.0|8.223|8.223|   1.0|0.092| 96.656|    0.0|\n",
      "|15.0|8.587|8.824| 370.0|8.587|8.824|   0.0|0.137|148.128|  370.0|\n",
      "|16.0|8.634|8.634| 350.0|8.616|8.616|   0.0|0.157|105.222|   20.0|\n",
      "|17.0|8.404|8.457|1250.0|8.404|8.441|   0.0|0.159| 67.586|-1230.0|\n",
      "|18.0|8.402|8.402|   0.0|8.402|8.402|   0.0|0.156| 54.927|-1230.0|\n",
      "|19.0|8.344|8.344|   0.0|8.344|8.344|   1.0|0.148| 39.074|-1230.0|\n",
      "|20.0|8.583|8.583|   0.0|8.583|8.583|   0.0|0.156| 70.221|-1230.0|\n",
      "|21.0|8.471|8.471|   0.0|8.471|8.471|   1.0|0.153| 43.572|-1230.0|\n",
      "|22.0|8.564|8.564|   0.0|8.564|8.564|   1.0|0.155| 54.995|-1230.0|\n",
      "|23.0|8.828|8.828|   0.0|8.828|8.828|   1.0|0.173| 106.49|-1230.0|\n",
      "|24.0|8.929|8.929|   0.0|8.929|8.929|   0.0|0.192|113.932|-1230.0|\n",
      "|25.0|8.902|8.902|   0.0|8.902|8.902|   1.0|0.202| 96.888|-1230.0|\n",
      "|26.0|9.092|9.092|   0.0|9.092|9.092|   0.0|0.222|124.932|-1230.0|\n",
      "|27.0|9.057|9.057|   0.0|9.057|9.057|   0.0|0.232| 97.596|-1230.0|\n",
      "|28.0| 8.93| 8.93|   0.0| 8.93| 8.93|   1.0|0.228|  59.03|-1230.0|\n",
      "|29.0|9.012|9.012|   0.0|9.012|9.012|   1.0|0.228| 72.028|-1230.0|\n",
      "|30.0|9.107|9.107|   0.0|9.107|9.107|   0.0|0.232| 82.117|-1230.0|\n",
      "|31.0|8.986|8.986|   0.0|8.986|8.986|   0.0|0.224| 45.815|-1230.0|\n",
      "|32.0|8.773|8.773|   0.0|8.773|8.773|   1.0|0.199|-14.703|-1230.0|\n",
      "+----+-----+-----+------+-----+-----+------+-----+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OBV indicator calculation\n",
    "if OBV_i:\n",
    "    temp_df = converted_df.toPandas()\n",
    "    df_obv = spark.createDataFrame(\n",
    "        temp_df.assign(OBV=(temp_df.Volume * (\n",
    "            ~temp_df.Close.diff().le(0) * 2 - 1)).cumsum()))\n",
    "    converted_df = df_obv.select(\n",
    "        [col(c).cast('float') for c in df_obv.columns])\n",
    "    if DEBUG:\n",
    "        converted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zapo/Documents/Spark/spark/lib/python3.6/site-packages/ipykernel_launcher.py:18: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=3,center=False).mean()\n",
      "/home/zapo/Documents/Spark/spark/lib/python3.6/site-packages/ipykernel_launcher.py:19: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=3,center=False).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+------+-----+-----+------+-----+-------+-------+------+\n",
      "|  id| Open| High|Volume|  Low|Close|Profit| MACD|    CCI|    OBV|   RSI|\n",
      "+----+-----+-----+------+-----+-----+------+-----+-------+-------+------+\n",
      "|16.0|8.634|8.634| 350.0|8.616|8.616|   0.0|0.157|105.222|   20.0|65.114|\n",
      "|17.0|8.404|8.457|1250.0|8.404|8.441|   0.0|0.159| 67.586|-1230.0|61.077|\n",
      "|18.0|8.402|8.402|   0.0|8.402|8.402|   0.0|0.156| 54.927|-1230.0|   0.0|\n",
      "|19.0|8.344|8.344|   0.0|8.344|8.344|   1.0|0.148| 39.074|-1230.0|   0.0|\n",
      "|20.0|8.583|8.583|   0.0|8.583|8.583|   0.0|0.156| 70.221|-1230.0|71.131|\n",
      "|21.0|8.471|8.471|   0.0|8.471|8.471|   1.0|0.153| 43.572|-1230.0|58.435|\n",
      "|22.0|8.564|8.564|   0.0|8.564|8.564|   1.0|0.155| 54.995|-1230.0|74.775|\n",
      "|23.0|8.828|8.828|   0.0|8.828|8.828|   1.0|0.173| 106.49|-1230.0|76.119|\n",
      "|24.0|8.929|8.929|   0.0|8.929|8.929|   0.0|0.192|113.932|-1230.0| 100.0|\n",
      "|25.0|8.902|8.902|   0.0|8.902|8.902|   1.0|0.202| 96.888|-1230.0|93.112|\n",
      "|26.0|9.092|9.092|   0.0|9.092|9.092|   0.0|0.222|124.932|-1230.0| 91.51|\n",
      "|27.0|9.057|9.057|   0.0|9.057|9.057|   0.0|0.232| 97.596|-1230.0|75.397|\n",
      "|28.0| 8.93| 8.93|   0.0| 8.93| 8.93|   1.0|0.228|  59.03|-1230.0|53.977|\n",
      "|29.0|9.012|9.012|   0.0|9.012|9.012|   1.0|0.228| 72.028|-1230.0|33.607|\n",
      "|30.0|9.107|9.107|   0.0|9.107|9.107|   0.0|0.232| 82.117|-1230.0|58.224|\n",
      "|31.0|8.986|8.986|   0.0|8.986|8.986|   0.0|0.224| 45.815|-1230.0|59.396|\n",
      "|32.0|8.773|8.773|   0.0|8.773|8.773|   1.0|0.199|-14.703|-1230.0|22.145|\n",
      "|33.0|8.816|8.816|   0.0|8.816|8.816|   1.0|0.181| -14.76|-1230.0|11.406|\n",
      "|34.0|9.042|9.042|   0.0|9.042|9.042|   0.0|0.181| 52.015|-1230.0|55.809|\n",
      "|35.0|8.943|8.943|   0.0|8.943|8.943|   0.0|0.172|  7.184|-1230.0|73.098|\n",
      "+----+-----+-----+------+-----+-----+------+-----+-------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RSI indicator calculaction\n",
    "if RSI_i:\n",
    "    converted_df = RSI(converted_df, 3, 'SMA')\n",
    "    converted_df = converted_df.filter(converted_df.RSI != \"NaN\")\n",
    "    if DEBUG:\n",
    "        converted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   df_date = df_date.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/Spark/spark/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2521\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2522\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2523\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d17ad8406294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     df_to_plot_dt = converted_df.select([\n\u001b[1;32m      4\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconverted_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Spark/spark/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mset_index\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   3144\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3145\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3146\u001b[0;31m                 \u001b[0mlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3147\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3148\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Spark/spark/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Spark/spark/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Spark/spark/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Spark/spark/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3837\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3838\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3839\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3840\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Spark/spark/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2522\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2523\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2524\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "# if DEBUG:\n",
    "#     df_date = df_date.set_index('id')\n",
    "#     df_to_plot_dt = converted_df.select([\n",
    "#         c for c in converted_df.columns\n",
    "#         if c not in\n",
    "#         {'OBV', 'Volume', 'Low', 'High', 'Open', 'Profit', 'RSI', 'CCI'}\n",
    "#     ])\n",
    "#     df_to_plot_dt = df_to_plot_dt.withColumn('Zero',lit(0))\n",
    "#     df_to_plot_dt = df_to_plot_dt.toPandas()\n",
    "#     df_to_plot_dt = df_to_plot_dt.set_index('id')\n",
    "#     df_to_plot_dt = df_to_plot_dt.join(df_date)\n",
    "#     plt_dt.figure(figsize=(24, 10))\n",
    "#     plt_dt.plot(df_to_plot_dt.Date, df_to_plot_dt.Close,df_to_plot_dt.Zero )\n",
    "#     plt_dt.legend(df_to_plot_dt.columns)\n",
    "#     plt_dt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+------+-----+-----+------+-----+-------+-------+------+\n",
      "|  id| Open| High|Volume|  Low|Close|Profit| MACD|    CCI|    OBV|   RSI|\n",
      "+----+-----+-----+------+-----+-----+------+-----+-------+-------+------+\n",
      "|16.0|8.634|8.634| 350.0|8.616|8.616|   0.0|0.157|105.222|   20.0|65.114|\n",
      "|17.0|8.404|8.457|1250.0|8.404|8.441|   0.0|0.159| 67.586|-1230.0|61.077|\n",
      "|18.0|8.402|8.402|   0.0|8.402|8.402|   0.0|0.156| 54.927|-1230.0|   0.0|\n",
      "|19.0|8.344|8.344|   0.0|8.344|8.344|   1.0|0.148| 39.074|-1230.0|   0.0|\n",
      "|20.0|8.583|8.583|   0.0|8.583|8.583|   0.0|0.156| 70.221|-1230.0|71.131|\n",
      "|21.0|8.471|8.471|   0.0|8.471|8.471|   1.0|0.153| 43.572|-1230.0|58.435|\n",
      "|22.0|8.564|8.564|   0.0|8.564|8.564|   1.0|0.155| 54.995|-1230.0|74.775|\n",
      "|23.0|8.828|8.828|   0.0|8.828|8.828|   1.0|0.173| 106.49|-1230.0|76.119|\n",
      "|24.0|8.929|8.929|   0.0|8.929|8.929|   0.0|0.192|113.932|-1230.0| 100.0|\n",
      "|25.0|8.902|8.902|   0.0|8.902|8.902|   1.0|0.202| 96.888|-1230.0|93.112|\n",
      "|26.0|9.092|9.092|   0.0|9.092|9.092|   0.0|0.222|124.932|-1230.0| 91.51|\n",
      "|27.0|9.057|9.057|   0.0|9.057|9.057|   0.0|0.232| 97.596|-1230.0|75.397|\n",
      "|28.0| 8.93| 8.93|   0.0| 8.93| 8.93|   1.0|0.228|  59.03|-1230.0|53.977|\n",
      "|29.0|9.012|9.012|   0.0|9.012|9.012|   1.0|0.228| 72.028|-1230.0|33.607|\n",
      "|30.0|9.107|9.107|   0.0|9.107|9.107|   0.0|0.232| 82.117|-1230.0|58.224|\n",
      "|31.0|8.986|8.986|   0.0|8.986|8.986|   0.0|0.224| 45.815|-1230.0|59.396|\n",
      "|32.0|8.773|8.773|   0.0|8.773|8.773|   1.0|0.199|-14.703|-1230.0|22.145|\n",
      "|33.0|8.816|8.816|   0.0|8.816|8.816|   1.0|0.181| -14.76|-1230.0|11.406|\n",
      "|34.0|9.042|9.042|   0.0|9.042|9.042|   0.0|0.181| 52.015|-1230.0|55.809|\n",
      "|35.0|8.943|8.943|   0.0|8.943|8.943|   0.0|0.172|  7.184|-1230.0|73.098|\n",
      "+----+-----+-----+------+-----+-----+------+-----+-------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "We have 1206 training examples and 130 test examples.\n",
      "+----+-----+-----+------+-----+-----+------+-----+-------+-------+------+\n",
      "|  id| Open| High|Volume|  Low|Close|Profit| MACD|    CCI|    OBV|   RSI|\n",
      "+----+-----+-----+------+-----+-----+------+-----+-------+-------+------+\n",
      "|16.0|8.634|8.634| 350.0|8.616|8.616|   0.0|0.157|105.222|   20.0|65.114|\n",
      "|17.0|8.404|8.457|1250.0|8.404|8.441|   0.0|0.159| 67.586|-1230.0|61.077|\n",
      "|18.0|8.402|8.402|   0.0|8.402|8.402|   0.0|0.156| 54.927|-1230.0|   0.0|\n",
      "|19.0|8.344|8.344|   0.0|8.344|8.344|   1.0|0.148| 39.074|-1230.0|   0.0|\n",
      "|21.0|8.471|8.471|   0.0|8.471|8.471|   1.0|0.153| 43.572|-1230.0|58.435|\n",
      "|22.0|8.564|8.564|   0.0|8.564|8.564|   1.0|0.155| 54.995|-1230.0|74.775|\n",
      "|23.0|8.828|8.828|   0.0|8.828|8.828|   1.0|0.173| 106.49|-1230.0|76.119|\n",
      "|24.0|8.929|8.929|   0.0|8.929|8.929|   0.0|0.192|113.932|-1230.0| 100.0|\n",
      "|25.0|8.902|8.902|   0.0|8.902|8.902|   1.0|0.202| 96.888|-1230.0|93.112|\n",
      "|26.0|9.092|9.092|   0.0|9.092|9.092|   0.0|0.222|124.932|-1230.0| 91.51|\n",
      "|27.0|9.057|9.057|   0.0|9.057|9.057|   0.0|0.232| 97.596|-1230.0|75.397|\n",
      "|28.0| 8.93| 8.93|   0.0| 8.93| 8.93|   1.0|0.228|  59.03|-1230.0|53.977|\n",
      "|29.0|9.012|9.012|   0.0|9.012|9.012|   1.0|0.228| 72.028|-1230.0|33.607|\n",
      "|30.0|9.107|9.107|   0.0|9.107|9.107|   0.0|0.232| 82.117|-1230.0|58.224|\n",
      "|31.0|8.986|8.986|   0.0|8.986|8.986|   0.0|0.224| 45.815|-1230.0|59.396|\n",
      "|32.0|8.773|8.773|   0.0|8.773|8.773|   1.0|0.199|-14.703|-1230.0|22.145|\n",
      "|33.0|8.816|8.816|   0.0|8.816|8.816|   1.0|0.181| -14.76|-1230.0|11.406|\n",
      "|34.0|9.042|9.042|   0.0|9.042|9.042|   0.0|0.181| 52.015|-1230.0|55.809|\n",
      "|35.0|8.943|8.943|   0.0|8.943|8.943|   0.0|0.172|  7.184|-1230.0|73.098|\n",
      "|37.0|8.927|8.927|   0.0|8.927|8.927|   0.0|0.152|-22.918|-1230.0|   0.0|\n",
      "+----+-----+-----+------+-----+-----+------+-----+-------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "converted_df = converted_df.sort(converted_df.id.asc())\n",
    "\n",
    "# Manual split for training and validating data\n",
    "if ManualSplit:\n",
    "    dfp = converted_df.toPandas()\n",
    "\n",
    "    dfp = np.array_split(dfp, 10)\n",
    "\n",
    "    p0 = spark.createDataFrame(data=dfp[0])\n",
    "    p1 = spark.createDataFrame(data=dfp[1])\n",
    "    p2 = spark.createDataFrame(data=dfp[2])\n",
    "    p3 = spark.createDataFrame(data=dfp[3])\n",
    "    p4 = spark.createDataFrame(data=dfp[4])\n",
    "    p5 = spark.createDataFrame(data=dfp[5])\n",
    "    p6 = spark.createDataFrame(data=dfp[6])\n",
    "    p7 = spark.createDataFrame(data=dfp[7])\n",
    "    p8 = spark.createDataFrame(data=dfp[8])\n",
    "    p9 = spark.createDataFrame(data=dfp[9])\n",
    "\n",
    "    p_final = p0.union(p1).union(p2).union(p3).union(p4).union(p5).union(\n",
    "        p6).union(p7).union(p8)\n",
    "    train = p_final\n",
    "    test = p9\n",
    "    #     test = p9.head(10)\n",
    "    #     test = spark.createDataFrame(test)\n",
    "else:\n",
    "    train, test = converted_df.randomSplit([0.9, 0.1], seed=RANDOM_SEED)\n",
    "\n",
    "print(\"We have %d training examples and %d test examples.\" % (train.count(),\n",
    "                                                              test.count()))\n",
    "test = test.select([col(c).cast('float') for c in test.columns])\n",
    "\n",
    "train = train.select([col(c).cast('float') for c in train.columns])\n",
    "\n",
    "test = test.sort(test.id.asc())\n",
    "\n",
    "train = train.sort(train.id.asc())\n",
    "\n",
    "if DEBUG:\n",
    "    train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "\n",
    "featuresCols = converted_df.columns\n",
    "featuresCols.remove('Profit')\n",
    "featuresCols.remove('id')\n",
    "\n",
    "print(featuresCols)\n",
    "\n",
    "# Vector Assembler\n",
    "# This concatenates all feature columns into a single feature vector in a new column \"rawFeatures\".\n",
    "# Used for assembling features into a vector.\n",
    "# We will pass all the columns that we are going to use for the prediction to the VectorAssembler and\n",
    "# it will create a new vector column.\n",
    "vectorAssembler_rt = VectorAssembler(\n",
    "    inputCols=featuresCols, outputCol=\"rawFeatures\")\n",
    "\n",
    "# VectorIndexer:\n",
    "# is used to index categorical predictors in a featuresCol column.\n",
    "# Remember that featuresCol is a single column consisting of vectors (refer to featuresCol and labelCol).\n",
    "# Each row is a vector which contains values from each predictors.\n",
    "\n",
    "featureIndexer_rt = VectorIndexer(\n",
    "    inputCol=\"rawFeatures\",\n",
    "    outputCol=\"features\",\n",
    "    maxCategories=len(featuresCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Algorithm\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "rt = DecisionTreeClassifier(\n",
    "    labelCol='Profit', featuresCol=\"features\", minInfoGain=0.01)\n",
    "max_Depth_Range = list(range(5, 20))\n",
    "min_InstancesPerNode = list(range(1, 5))\n",
    "\n",
    "paramGrid_rt = ParamGridBuilder() \\\n",
    "    .addGrid(rt.maxDepth, max_Depth_Range) \\\n",
    "    .addGrid(rt.maxMemoryInMB, [1000] ).build()\n",
    "\n",
    "# We define an evaluation metric. This tells CrossValidator how well we are doing by comparing the true\n",
    "# labels with predictions.\n",
    "\n",
    "evaluator_rt = BinaryClassificationEvaluator(\n",
    "    labelCol=rt.getLabelCol(),\n",
    "    metricName='areaUnderROC',\n",
    "    rawPredictionCol=rt.getRawPredictionCol())\n",
    "\n",
    "evaluator_rt_PR = BinaryClassificationEvaluator(\n",
    "    labelCol=rt.getLabelCol(),\n",
    "    metricName='areaUnderPR',\n",
    "    rawPredictionCol=rt.getRawPredictionCol())\n",
    "\n",
    "# Declare the CrossValidator, which runs model tuning for us.\n",
    "cv_rt = CrossValidator(\n",
    "    estimator=rt,\n",
    "    evaluator=evaluator_rt,\n",
    "    estimatorParamMaps=paramGrid_rt,\n",
    "    numFolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "\n",
    "DecisionTreeClassificationModel.getMaxDepth = (\n",
    "    lambda self: self._java_obj.getMaxDepth())\n",
    "\n",
    "DecisionTreeClassificationModel.getMinInstancesPerNode = (\n",
    "    lambda self: self._java_obj.getMinInstancesPerNode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# if DEBUG:\n",
    "#     train.repartition(1).write.csv(\"PreProcessedSets/TrainSet_\" + str(time.mktime(datetime.datetime.today().timetuple())) + \"_.csv\", header = 'True')\n",
    "#     test.repartition(1).write.csv(\"PreProcessedSets/TestSet_\" + str(time.mktime(datetime.datetime.today().timetuple())) + \"_.csv\", header = 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Creating Final pipeline object\n",
    "pipeline_rt = Pipeline(stages=[vectorAssembler_rt, featureIndexer_rt, cv_rt])\n",
    "\n",
    "# FITTING!\n",
    "pipelineModel_rt = pipeline_rt.fit(train)\n",
    "\n",
    "# Getting the Best Model\n",
    "best_classifier = pipelineModel_rt.stages[-1].bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "print('Features importances' + str(best_classifier.featureImportances))\n",
    "\n",
    "# Max depth\n",
    "print(\"Maximal depth is \" + str(best_classifier.getMaxDepth()))\n",
    "max_depth = best_classifier.getMaxDepth()\n",
    "\n",
    "# Min instances\n",
    "print(\"Minimal instances per node is \" + str(\n",
    "    best_classifier.getMinInstancesPerNode()))\n",
    "min_instancesPerNode = best_classifier.getMinInstancesPerNode()\n",
    "\n",
    "# Making Predictions!\n",
    "predictions_rt = pipelineModel_rt.transform(test)\n",
    "\n",
    "# Calculating metrics\n",
    "AreaUnderROC = evaluator_rt.evaluate(predictions_rt)\n",
    "print(\"AreaUnderROC on our test set: %g\" % AreaUnderROC)\n",
    "\n",
    "# Calculating metrics\n",
    "AreaUnderPR = evaluator_rt_PR.evaluate(predictions_rt)\n",
    "print(\"AreaUnderPR on our test set: %g\" % AreaUnderPR)\n",
    "\n",
    "#evaluate results\n",
    "testCount = predictions_rt.count()\n",
    "\n",
    "FP = predictions_rt.where(\"prediction = 0 AND Profit=1\").count()  #FN\n",
    "FN = predictions_rt.where(\"prediction = 1 AND Profit=0\").count()  #TN\n",
    "TP = predictions_rt.where(\"prediction = 1 AND Profit=1\").count()  #TP\n",
    "TN = predictions_rt.where(\"prediction = 0 AND Profit=0\").count()  #TN\n",
    "\n",
    "print(\"Test count | FN | TN | TP | TN\")\n",
    "print(\n",
    "    str(testCount) + \" | \" + str(FP) + \" | \" + str(FN) + \" | \" + str(TP) +\n",
    "    \" | \" + str(TN))\n",
    "\n",
    "# predictions_rt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model = pipelineModel_rt\n",
    "from random import *\n",
    "\n",
    "for i in range(10):\n",
    "    new_train, new_test = converted_df.randomSplit([0.9, 0.1], seed=i + 1)\n",
    "    new_test = new_test.sort(new_test.id.asc())\n",
    "\n",
    "    predictions = pipelineModel_rt.transform(new_test)\n",
    "\n",
    "    # Calculating metrics\n",
    "    AreaUnderROC = evaluator_rt.evaluate(predictions)\n",
    "    print(\"AreaUnderROC on our test set: %g\" % AreaUnderROC)\n",
    "\n",
    "    # Calculating metrics\n",
    "    AreaUnderPR = evaluator_rt_PR.evaluate(predictions)\n",
    "    print(\"AreaUnderPR on our test set: %g\" % AreaUnderPR)\n",
    "\n",
    "    #evaluate results\n",
    "    testCount = predictions_rt.count()\n",
    "\n",
    "    FP = predictions_rt.where(\"prediction = 0 AND Profit=1\").count()  #FN\n",
    "    FN = predictions_rt.where(\"prediction = 1 AND Profit=0\").count()  #TN\n",
    "    TP = predictions_rt.where(\"prediction = 1 AND Profit=1\").count()  #TP\n",
    "    TN = predictions_rt.where(\"prediction = 0 AND Profit=0\").count()  #TN\n",
    "\n",
    "    print(\"Test count | FN | TN | TP | TN\")\n",
    "    print(\n",
    "        str(testCount) + \" | \" + str(FP) + \" | \" + str(FN) + \" | \" + str(TP) +\n",
    "        \" | \" + str(TN))\n",
    "\n",
    "    print(\"####################################################\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "test = predictions_rt.toPandas()\n",
    "csv_buffer = StringIO()\n",
    "test.to_csv(csv_buffer)\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "s3_resource.Object('logs102', 'DT_Final.csv').put(Body=csv_buffer.getvalue())\n",
    "\n",
    "if DEBUG != True:\n",
    "    df_to_plot_rt = predictions_rt.select('prediction', 'Profit')\n",
    "    print(df_to_plot_rt)\n",
    "    df_to_plot_rt = df_to_plot_rt.toPandas()\n",
    "    plt_dt.figure(figsize=(14, 14))\n",
    "    plt_dt.plot(df_to_plot_rt)\n",
    "    plt_dt.legend(df_to_plot_rt.columns)\n",
    "    plt_dt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
