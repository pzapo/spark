{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import lit, udf, monotonically_increasing_id, col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer, StringIndexer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "# Algos# Algorithm\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Others\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Graphs libs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt_dt\n",
    "\n",
    "# Supress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# S3 Service\n",
    "# import boto3\n",
    "# from io import StringIO\n",
    "# # Let's use Amazon S3\n",
    "# s3 = boto3.resource('s3')\n",
    "\n",
    "# Spark context simple configuration\n",
    "spark = SparkSession.builder.config(conf=SparkConf()).getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helpers.technical_indicators import RSI, CCI, MACD\n",
    "from Helpers.generated_features import features_from_OHLC\n",
    "from Helpers.CustomTS import TrainValidationSplitSorted\n",
    "from Helpers.best_model_params import *\n",
    "\n",
    "from Helpers.udf import profit_\n",
    "profit_udf = udf(profit_, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "OBV_i = True\n",
    "CCI_i = True\n",
    "RSI_i = True\n",
    "MACD_i = True\n",
    "\n",
    "ManualSplit = True\n",
    "SORT = True\n",
    "CHUNKS = 10\n",
    "\n",
    "Date_Convert = False\n",
    "\n",
    "CV = False\n",
    "\n",
    "DEBUG = False\n",
    "RANDOM_SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "#Path to file with CSV\n",
    "\n",
    "# path_to_csv = \"s3://stocksets100/Orlen.csv\"\n",
    "path_to_csv = \"./Datasets/KGHA.csv\"\n",
    "\n",
    "fresh_df = spark.read.csv(path_to_csv, header=True, inferSchema=True)\n",
    "\n",
    "fresh_df = fresh_df.filter(fresh_df.Open != \"null\")\n",
    "\n",
    "temporary_df = fresh_df.select(\n",
    "    fresh_df[\"Date\"].cast(\"Date\"), fresh_df[\"Open\"].cast(\"float\"),\n",
    "    fresh_df[\"High\"].cast(\"float\"), fresh_df[\"Volume\"].cast(\"int\"),\n",
    "    fresh_df[\"Low\"].cast(\"float\"), fresh_df[\"Close\"].cast(\"float\"))\n",
    "\n",
    "# id\n",
    "temporary_df = temporary_df.select(\"*\").withColumn(\n",
    "    \"id\", monotonically_increasing_id())\n",
    "\n",
    "featured_df = features_from_OHLC(temporary_df, spark)\n",
    "featured_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Creating new column with shifted Close price by 1 day\n",
    "df_daily_return = featured_df.withColumn('prev_day_price',\n",
    "                                         F.lag(featured_df['Close']).over(\n",
    "                                             Window.orderBy(\"id\")))\n",
    "\n",
    "df_daily_return = df_daily_return.filter(\n",
    "    df_daily_return.prev_day_price.isNotNull())\n",
    "\n",
    "# Profit label calculation\n",
    "# 1 if stock risen up, 0 is it went down\n",
    "df_profit = df_daily_return.withColumn(\n",
    "    'Profit', profit_udf(df_daily_return.Close,\n",
    "                         df_daily_return.prev_day_price))\n",
    "\n",
    "df_shifted_profit = df_profit.withColumn(\n",
    "    'Profit',\n",
    "    F.lag(df_profit['Profit'], count=-1).over(Window.orderBy(\"Date\")))\n",
    "\n",
    "final_df = df_shifted_profit.filter(df_shifted_profit.Profit.isNotNull())\n",
    "\n",
    "# Removing redudant columns\n",
    "final_df = final_df.drop(\"Daily return\")\n",
    "final_df = final_df.drop(\"prev_day_price\")\n",
    "\n",
    "if DEBUG:\n",
    "    final_df.show(2)\n",
    "    \n",
    "converted_df = final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "#Columns without Date\n",
    "# converted_df = converted_df.select(\n",
    "#     [col(c).cast('float') for c in converted_df.columns if c. not in {'Date'}])\n",
    "\n",
    "# Date column!\n",
    "df_date = converted_df.select(converted_df.Date)\n",
    "df_date = df_date.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "#Convert date to splitted format\n",
    "if Date_Convert:\n",
    "    split_col = pyspark.sql.functions.split(converted_df['Date'], '-')\n",
    "    converted_df = converted_df.withColumn('Year',\n",
    "                                           split_col.getItem(0).cast('int'))\n",
    "    converted_df = converted_df.withColumn('Month',\n",
    "                                           split_col.getItem(1).cast('int'))\n",
    "    converted_df = converted_df.withColumn('Day',\n",
    "                                           split_col.getItem(2).cast('int'))\n",
    "    if DEBUG:\n",
    "        converted_df.show()\n",
    "\n",
    "converted_df = converted_df.drop(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "if MACD_i:\n",
    "    converted_df = MACD(converted_df, spark)\n",
    "    converted_df = converted_df.select(\n",
    "        [col(c).cast('float') for c in converted_df.columns])\n",
    "    if DEBUG:\n",
    "        converted_df.show()\n",
    "\n",
    "if CCI_i:\n",
    "    converted_df = CCI(converted_df, 14, spark)\n",
    "    converted_df = converted_df.select(\n",
    "        [col(c).cast('float') for c in converted_df.columns])\n",
    "    converted_df = converted_df.filter(converted_df.CCI != \"NaN\")\n",
    "    if DEBUG:\n",
    "        converted_df.show()\n",
    "\n",
    "if OBV_i:\n",
    "    temp_df = converted_df.toPandas()\n",
    "    df_obv = spark.createDataFrame(\n",
    "        temp_df.assign(OBV=(temp_df.Volume * (\n",
    "            ~temp_df.Close.diff().le(0) * 2 - 1)).cumsum()))\n",
    "    converted_df = df_obv.select(\n",
    "        [col(c).cast('float') for c in df_obv.columns])\n",
    "    if DEBUG:\n",
    "        converted_df.show()\n",
    "\n",
    "if RSI_i:\n",
    "    converted_df = RSI(converted_df, 3, 'EWMA', spark)\n",
    "    converted_df = converted_df.filter(converted_df.RSI != \"NaN\")\n",
    "    if DEBUG:\n",
    "        converted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "converted_df = converted_df.sort(converted_df.id.asc())\n",
    "\n",
    "if DEBUG:\n",
    "    print(converted_df.count())\n",
    "\n",
    "# Manual split for training and validating data\n",
    "if ManualSplit:\n",
    "    dfp = converted_df.toPandas()\n",
    "    dfp = np.array_split(dfp, CHUNKS)\n",
    "    train = spark.createDataFrame(data=dfp[0].round(3))\n",
    "    for i in range(1, len(dfp) - 1):\n",
    "        p = spark.createDataFrame(data=dfp[i].round(3))\n",
    "        train = train.union(p)\n",
    "    test = spark.createDataFrame(data=dfp[-1].round(3))\n",
    "else:\n",
    "    train, test = converted_df.randomSplit([0.9, 0.1], seed=RANDOM_SEED)\n",
    "\n",
    "print(\"We have %d training examples and %d test examples.\" % (train.count(),\n",
    "                                                              test.count()))\n",
    "if SORT:\n",
    "    test = test.sort(test.id.asc())\n",
    "    train = train.sort(train.id.asc())\n",
    "\n",
    "if not DEBUG:\n",
    "    train.show()\n",
    "    test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "featuresCols = converted_df.columns\n",
    "featuresCols.remove('Profit')\n",
    "featuresCols.remove('id')\n",
    "\n",
    "print(featuresCols)\n",
    "\n",
    "# Vector Assembler\n",
    "# This concatenates all feature columns into a single feature vector in a new column \"rawFeatures\".\n",
    "# Used for assembling features into a vector.\n",
    "# We will pass all the columns that we are going to use for the prediction to the VectorAssembler and\n",
    "# it will create a new vector column.\n",
    "vectorAssembler_rt = VectorAssembler(\n",
    "    inputCols=featuresCols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Patrameters grid testing\n",
    "# rt = DecisionTreeClassifier(\n",
    "#     labelCol='Profit', featuresCol=\"features\", minInfoGain=0.01,  maxBins=200)\n",
    "\n",
    "rt = RandomForestClassifier(\n",
    "    labelCol='Profit', featuresCol=\"features\", numTrees=25, maxBins=300)\n",
    "\n",
    "max_Depth_Range = list(range(5, 15))\n",
    "min_InstancesPerNode = list(range(5, 15))\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rt.maxDepth, max_Depth_Range) \\\n",
    "    .addGrid(rt.maxMemoryInMB, [1000] ).build()\n",
    "\n",
    "# We define an evaluation metric. This tells Validator how well we are doing by comparing the true\n",
    "# labels with predictions.\n",
    "\n",
    "multi = MulticlassClassificationEvaluator(\n",
    "    labelCol=rt.getLabelCol(),\n",
    "    metricName='accuracy',\n",
    "    predictionCol=rt.getPredictionCol())\n",
    "\n",
    "evaluator_rt = BinaryClassificationEvaluator(\n",
    "    labelCol=rt.getLabelCol(),\n",
    "    metricName='areaUnderROC',\n",
    "    rawPredictionCol=rt.getRawPredictionCol())\n",
    "\n",
    "evaluator_rt_PR = BinaryClassificationEvaluator(\n",
    "    labelCol=rt.getLabelCol(),\n",
    "    metricName='areaUnderPR',\n",
    "    rawPredictionCol=rt.getRawPredictionCol())\n",
    "\n",
    "# Declare the CrossValidator, which runs model tuning for us.\n",
    "if CV:\n",
    "    val = CrossValidator(\n",
    "        estimator=rt,\n",
    "        evaluator=multi,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        numFolds=2)\n",
    "else:\n",
    "    val = TrainValidationSplitSorted(\n",
    "        chunks = CHUNKS,\n",
    "        spark = spark,\n",
    "        estimator=rt,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Creating Final pipeline object\n",
    "pipeline_rt = Pipeline(stages=[vectorAssembler_rt, val])\n",
    "\n",
    "# FITTING!\n",
    "pipelineModel_rt = pipeline_rt.fit(train)\n",
    "\n",
    "# Getting the Best Model\n",
    "best_classifier = pipelineModel_rt.stages[-1].bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "print('Features importances' + str(best_classifier.featureImportances))\n",
    "final_features = best_classifier.featureImportances\n",
    "   \n",
    "for feature, importance in zip(featuresCols, final_features):\n",
    "    print(\"{} - {}\".format(feature, round(importance, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Model Parameters!\n",
    "# Max depth\n",
    "print(\"Maximal depth is \" + str(best_classifier.getMaxDepth()))\n",
    "max_depth = best_classifier.getMaxDepth()\n",
    "\n",
    "# Min instances\n",
    "print(\"Minimal instances per node is \" + str(\n",
    "    best_classifier.getMinInstancesPerNode()))\n",
    "min_instancesPerNode = best_classifier.getMinInstancesPerNode()\n",
    "\n",
    "# Making Predictions!\n",
    "predictions_rt = pipelineModel_rt.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy = multi.evaluate(predictions_rt)\n",
    "print(\"Accuracy is equal to {}%\".format(round(accuracy, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Calculating metrics\n",
    "AreaUnderROC = evaluator_rt.evaluate(predictions_rt)\n",
    "print(\"AreaUnderROC on our test set: %g\" % AreaUnderROC)\n",
    "\n",
    "# Calculating metrics\n",
    "AreaUnderPR = evaluator_rt_PR.evaluate(predictions_rt)\n",
    "print(\"AreaUnderPR on our test set: %g\" % AreaUnderPR)\n",
    "\n",
    "#evaluate results\n",
    "testCount = predictions_rt.count()\n",
    "\n",
    "FP = predictions_rt.where(\"prediction = 1 AND Profit = 0\").count()\n",
    "FN = predictions_rt.where(\"prediction = 0 AND Profit = 1\").count()\n",
    "TP = predictions_rt.where(\"prediction = 1 AND Profit = 1\").count()\n",
    "TN = predictions_rt.where(\"prediction = 0 AND Profit = 0\").count()\n",
    "\n",
    "print(\"Count | FP | FN | TP | TN\")\n",
    "print(\n",
    "    str(testCount) + \" | \" + str(FP) + \" | \" + str(FN) + \" | \" + str(TP) +\n",
    "    \" | \" + str(TN))\n",
    "\n",
    "# predictions_rt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "test = predictions_rt.toPandas()\n",
    "# csv_buffer = StringIO()\n",
    "# test.to_csv(csv_buffer)\n",
    "\n",
    "# s3_resource = boto3.resource('s3')\n",
    "\n",
    "# s3_resource.Object('logs102', 'DT_Final.csv').put(Body=csv_buffer.getvalue())\n",
    "\n",
    "if DEBUG != True:\n",
    "    df_to_plot_rt = predictions_rt.select('prediction', 'Profit')\n",
    "    print(df_to_plot_rt)\n",
    "    df_to_plot_rt = df_to_plot_rt.toPandas()\n",
    "    plt_dt.figure(figsize=(24, 3))\n",
    "    plt_dt.plot(df_to_plot_rt)\n",
    "    plt_dt.legend(df_to_plot_rt.columns)\n",
    "    plt_dt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# final_model = pipelineModel_rt\n",
    "from random import *\n",
    "\n",
    "for i in range(30):\n",
    "    new_train, new_test = converted_df.randomSplit([0.1, 0.9], seed=i + 1)\n",
    "    new_test = new_test.sort(new_test.id.asc())\n",
    "\n",
    "    predictions = pipelineModel_rt.transform(new_test)\n",
    "\n",
    "    # Calculating metrics\n",
    "    AreaUnderROC = evaluator_rt.evaluate(predictions)\n",
    "    print(\"AreaUnderROC on our test set: %g\" % AreaUnderROC)\n",
    "\n",
    "    # Calculating metrics\n",
    "    AreaUnderPR = evaluator_rt_PR.evaluate(predictions)\n",
    "    print(\"AreaUnderPR on our test set: %g\" % AreaUnderPR)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = multi.evaluate(predictions)\n",
    "    print(\"Accuracy is equal to {}%\".format(round(accuracy, 3)))\n",
    "\n",
    "    #evaluate results\n",
    "    testCount = predictions.count()\n",
    "\n",
    "    FP = predictions.where(\"prediction = 1 AND Profit = 0\").count()\n",
    "    FN = predictions.where(\"prediction = 0 AND Profit = 1\").count()   \n",
    "    TP = predictions.where(\"prediction = 1 AND Profit = 1\").count() \n",
    "    TN = predictions.where(\"prediction = 0 AND Profit = 0\").count()\n",
    "\n",
    "    print(\"Count | FP | FN | TP | TN\")\n",
    "    print(\n",
    "        str(testCount) + \" | \" + str(FP) + \" | \" + str(FN) + \" | \" + str(TP) +\n",
    "        \" | \" + str(TN))\n",
    "\n",
    "    print(\"####################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
