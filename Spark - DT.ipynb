{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer, StringIndexer, Normalizer, StandardScaler, MaxAbsScaler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Algorithms\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, LinearSVC, RandomForestClassifier\n",
    "\n",
    "\n",
    "# Others\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Graphs libs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "# Spark context simple configuration\n",
    "conf = SparkConf()\n",
    "conf.setAppName('ipython-notebook').set(\"spark.driver.memory\", \"4g\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "spark.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from Helpers.technical_indicators import calc_ti\n",
    "from Helpers.generated_features import features_from_OHLC\n",
    "from Helpers.CustomTS import TrainValidationSplitSorted\n",
    "from Helpers.best_model_params import *\n",
    "from Helpers.Models import *\n",
    "from Helpers.parse import tree_json\n",
    "\n",
    "from ProcessingData.processing import *\n",
    "from Stats.measures import *\n",
    "from Helpers.udf import BuyAndHoldClassifier, ReverseTradeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "ManualSplit = False\n",
    "\n",
    "TRAIN_FOLD = 7\n",
    "TEST_FOLD= 3\n",
    "\n",
    "SORTED = False\n",
    "\n",
    "RANDOM_SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "path = \"./Datasets/WIG20.csv\"\n",
    "df = complete_processing(spark, path)\n",
    "\n",
    "train, test = train_test_split(spark, df, TRAIN_FOLD, TEST_FOLD, ManualSplit, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "featuresCols = df.columns\n",
    "featuresCols.remove('Profit')\n",
    "featuresCols.remove('id')\n",
    "print(featuresCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Patrameters grid testing\n",
    "classifier, paramGrid = getDecisonTreewithGrid(max_Bins=250,\n",
    "                                               min_InstancesPerNode=[1],\n",
    "                                               max_Depth_Range=[6,7],\n",
    "                                                min_infoGain=[0.000001])\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=classifier.getLabelCol(),\n",
    "    metricName='accuracy',\n",
    "    predictionCol=classifier.getPredictionCol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Declare the CrossValidator, which runs model tuning for us.\n",
    "if SORTED:\n",
    "    validator = TrainValidationSplitSorted(\n",
    "    train_fold=TRAIN_FOLD,\n",
    "    test_fold=TEST_FOLD,\n",
    "    spark=spark,\n",
    "    estimator=classifier,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator)\n",
    "else:\n",
    "    validator = TrainValidationSplit(\n",
    "    trainRatio = 0.7,\n",
    "    estimator=classifier,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler(inputCol=\"rawFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Vector Assembler\n",
    "# This concatenates all feature columns into a single feature vector in a new column \"rawFeatures\".\n",
    "# Used for assembling features into a vector.\n",
    "# We will pass all the columns that we are going to use for the prediction to the VectorAssembler and\n",
    "# it will create a new vector column.\n",
    "# Creating Final pipeline object\n",
    "if scaler is not None:\n",
    "    vectorAssembler_rt = VectorAssembler(\n",
    "    inputCols=featuresCols, outputCol=\"rawFeatures\")\n",
    "    pipeline = Pipeline(stages=[vectorAssembler_rt,scaler, validator])\n",
    "else:\n",
    "    vectorAssembler_rt = VectorAssembler(\n",
    "    inputCols=featuresCols, outputCol=\"features\")\n",
    "    pipeline = Pipeline(stages=[vectorAssembler_rt, validator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# FITTING!\n",
    "import time\n",
    "print(\"Training Started!\")\n",
    "start = time.time()\n",
    "pipelineModel = pipeline.fit(train)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "print(\"Training finished in: \" + str( round((end - start)/60)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Getting the Best Model\n",
    "best_classifier = pipelineModel.stages[-1].bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "best_tree_par(best_classifier)\n",
    "tree_feature_importances(best_classifier,featuresCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Making Predictions!\n",
    "predictions = pipelineModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "#evaluate results\n",
    "calc_metrics(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "df_to_plot_rt = predictions.select('prediction', 'Profit')\n",
    "df_to_plot_rt = df_to_plot_rt.toPandas()\n",
    "plt.figure(figsize=(24, 3))\n",
    "plt.plot(df_to_plot_rt)\n",
    "plt.legend(df_to_plot_rt.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import randint\n",
    "i = 0\n",
    "# path_to_csv = \"s3://stocksets100/Orlen.csv\"\n",
    "path = \"./Datasets/WIG20Verify.csv\"\n",
    "df = complete_processing(spark, path)\n",
    "ManualSplit = False\n",
    "results_MC = []\n",
    "Mlavg_a = []\n",
    "Mlavg_se = []\n",
    "Mlavg_sp = []\n",
    "Mlavg_p = []\n",
    "i = 0\n",
    "if scaler is not None:\n",
    "    pipeline_test = Pipeline(stages=[vectorAssembler_rt,scaler, best_classifier])\n",
    "else:\n",
    "    pipeline_test = Pipeline(stages=[vectorAssembler_rt, best_classifier])\n",
    "\n",
    "while len(results_MC) != 30:\n",
    "    train, test = validate(df, RANDOM_SEED + i)\n",
    "    predictions = pipeline_test.fit(test).transform(test)\n",
    "    temp_dcit = get_metrics(predictions,0.1)\n",
    "    if temp_dcit is None:\n",
    "        continue\n",
    "    results_MC.append(temp_dcit)\n",
    "    Mlavg_a.append(temp_dcit['accuracy'])\n",
    "    Mlavg_se.append(temp_dcit['sensitivity'])\n",
    "    Mlavg_sp.append(temp_dcit['specificity'])\n",
    "    Mlavg_p.append(temp_dcit['precision'])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(list(range(1, 31)),Mlavg_a)\n",
    "plt.xlabel('Nr próbki')\n",
    "plt.ylabel('Jakość w %')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "avg_a = sum(Mlavg_a)/len(Mlavg_a)\n",
    "print(round(avg_a,2))\n",
    "\n",
    "avg_se = sum(Mlavg_se)/len(Mlavg_se)\n",
    "print(round(avg_se,2))\n",
    "\n",
    "avg_sp = sum(Mlavg_sp)/len(Mlavg_sp)\n",
    "print(round(avg_sp,2))\n",
    "\n",
    "avg_p = sum(Mlavg_p)/len(Mlavg_p)\n",
    "print(round(avg_p,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "k2, p = stats.normaltest(Mlavg_a)\n",
    "alpha = 5e-2\n",
    "print(p)\n",
    "if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "    print(\"The null hypothesis can be rejected\")\n",
    "else:\n",
    "    print(\"The null hypothesis cannot be rejected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.458,0.472,0.444,0.465,0.451\n",
      "0.488,0.494,0.482,0.496,0.48\n",
      "0.487,0.498,0.477,0.483,0.492\n",
      "0.499,0.496,0.502,0.544,0.455\n",
      "0.468,0.451,0.486,0.488,0.449\n",
      "0.469,0.453,0.486,0.484,0.455\n",
      "0.488,0.481,0.496,0.502,0.475\n",
      "0.47,0.44,0.504,0.5,0.444\n",
      "0.481,0.478,0.485,0.52,0.442\n",
      "0.472,0.477,0.467,0.502,0.442\n",
      "0.468,0.476,0.459,0.481,0.453\n",
      "0.478,0.491,0.464,0.498,0.457\n",
      "0.5,0.479,0.519,0.473,0.525\n",
      "0.46,0.427,0.494,0.47,0.451\n",
      "0.468,0.43,0.506,0.472,0.464\n",
      "0.498,0.5,0.496,0.502,0.494\n",
      "0.51,0.483,0.537,0.516,0.504\n",
      "0.483,0.463,0.504,0.509,0.459\n",
      "0.499,0.494,0.504,0.53,0.469\n",
      "0.491,0.494,0.487,0.516,0.466\n",
      "0.489,0.471,0.506,0.475,0.502\n",
      "0.495,0.471,0.519,0.488,0.502\n",
      "0.489,0.494,0.485,0.479,0.5\n",
      "0.473,0.445,0.5,0.462,0.484\n",
      "0.502,0.514,0.49,0.508,0.496\n",
      "0.482,0.472,0.494,0.5,0.466\n",
      "0.479,0.461,0.498,0.488,0.471\n",
      "0.482,0.459,0.506,0.492,0.473\n",
      "0.475,0.46,0.493,0.519,0.434\n",
      "0.479,0.489,0.469,0.469,0.489\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "i = 0\n",
    "avg_accuracy = 0\n",
    "avg_sensitivity = 0\n",
    "avg_specificity = 0\n",
    "avg_precision = 0\n",
    "path = \"./Datasets/OrlenVerify.csv\"\n",
    "ManualSplit = False\n",
    "results_BC = []\n",
    "Blavg_a = []\n",
    "Blavg_se = []\n",
    "Blavg_sp = []\n",
    "Blavg_p = []\n",
    "\n",
    "while len(results_BC) != 30:\n",
    "    df = simple_processing(spark, path)\n",
    "    train, test  = validate(df, RANDOM_SEED + i)\n",
    "    temp_dcit = get_metrics(test,0.0)\n",
    "    if temp_dcit is None:\n",
    "        continue\n",
    "    results_BC.append(temp_dcit)\n",
    "    Blavg_a.append(temp_dcit['accuracy'])\n",
    "    Blavg_se.append(temp_dcit['sensitivity'])\n",
    "    Blavg_sp.append(temp_dcit['specificity'])\n",
    "    Blavg_p.append(temp_dcit['precision'])\n",
    "    i+=1\n",
    "    \n",
    "    if len(results_BC) == 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(list(range(1, 31)),Blavg_a)\n",
    "plt.xlabel('Nr próbki')\n",
    "plt.ylabel('Jakość w %')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "Bavg_a = sum(Blavg_a)/len(Blavg_a)\n",
    "print(round(Bavg_a,2))\n",
    "\n",
    "Bavg_se = sum(Blavg_se)/len(Blavg_se)\n",
    "print(round(Bavg_se,2))\n",
    "\n",
    "Bavg_sp = sum(Blavg_sp)/len(Blavg_sp)\n",
    "print(round(Bavg_sp,2))\n",
    "\n",
    "Bavg_p = sum(Blavg_p)/len(Blavg_p)\n",
    "print(round(Bavg_p,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "k2, p = stats.normaltest(Blavg_a)\n",
    "alpha = 5e-2\n",
    "print(p)\n",
    "if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "    print(\"The null hypothesis can be rejected\")\n",
    "else:\n",
    "    print(\"The null hypothesis cannot be rejected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "w, p = stats.levene(Mlavg_a,Blavg_a)\n",
    "alpha = 5e-2\n",
    "print(p)\n",
    "if p < alpha:  # null hypothesis: \n",
    "    print(\"The null hypothesis can be rejected\")\n",
    "else:\n",
    "    print(\"The null hypothesis cannot be rejected\")\n",
    "    \n",
    "s, p = stats.ttest_rel(Mlavg_a,Blavg_a)\n",
    "alpha = 5e-2\n",
    "print(p)\n",
    "if p < alpha:  # null hypothesis: \n",
    "    print(\"The null hypothesis can be rejected\")\n",
    "else:\n",
    "    print(\"The null hypothesis cannot be rejected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# stats_df = pd.DataFrame( {'Accuracy_MC' : results})\n",
    "stats_df = pd.DataFrame( {'Accuracy_BC' : Blavg_a ,'Accuracy_MC' : Mlavg_a})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "stats_df.plot.box(figsize=(10, 10))\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.4,0.8])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
